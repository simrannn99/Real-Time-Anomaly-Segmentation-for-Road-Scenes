{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ET9rEghpeE8f",
        "outputId": "349fcefd-afcc-4f78-e8a3-182e088b205d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Real-Time-Anomaly-Segmentation-for-Road-Scenes'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 236 (delta 26), reused 56 (delta 19), pack-reused 171 (from 1)\u001b[K\n",
            "Receiving objects: 100% (236/236), 98.94 MiB | 16.04 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes\n",
            "Branch 'evaluation_erfnet' set up to track remote branch 'evaluation_erfnet' from 'origin'.\n",
            "Switched to a new branch 'evaluation_erfnet'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# download the github repo\n",
        "!git clone https://github.com/simrannn99/Real-Time-Anomaly-Segmentation-for-Road-Scenes.git\n",
        "# Move into the repository directory\n",
        "%cd Real-Time-Anomaly-Segmentation-for-Road-Scenes\n",
        "\n",
        "# Checkout the specific branch\n",
        "#!git checkout evaluation_erfnet\n",
        "\n",
        "# Move all files to the parent directory (optional, if needed)\n",
        "!mv * ..\n",
        "\n",
        "# Navigate back to the parent directory (if files are moved)\n",
        "%cd ..\n",
        "\n",
        "# Remove the cloned directory (optional, if files were moved)\n",
        "!rm -r Real-Time-Anomaly-Segmentation-for-Road-Scenes/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6iDwhbgk1N4C",
        "outputId": "2368a613-1256-49af-b031-d7c93067dfda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cityscapesscripts\n",
            "  Downloading cityscapesScripts-2.2.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts) (3.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts) (11.0.0)\n",
            "Collecting appdirs (from cityscapesscripts)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyquaternion (from cityscapesscripts)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting coloredlogs (from cityscapesscripts)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts) (4.67.1)\n",
            "Collecting typing (from cityscapesscripts)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts) (2.32.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->cityscapesscripts)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->cityscapesscripts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->cityscapesscripts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->cityscapesscripts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->cityscapesscripts) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->cityscapesscripts) (1.17.0)\n",
            "Downloading cityscapesScripts-2.2.4-py3-none-any.whl (473 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.6/473.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26303 sha256=e141cec6d7858bbf42f6c4dcd16c36a4d5fd7e4656c8d9c6b2e51d1815cc767a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "Successfully built typing\n",
            "Installing collected packages: appdirs, typing, pyquaternion, humanfriendly, coloredlogs, cityscapesscripts\n",
            "Successfully installed appdirs-1.4.4 cityscapesscripts-2.2.4 coloredlogs-15.0.1 humanfriendly-10.0 pyquaternion-0.9.9 typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              },
              "id": "49bbc7b204a34646aacce901c8ff5fdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cityscapes username or email address: s303369\n",
            "Cityscapes password: \n",
            "Store credentials unencrypted in '/root/.local/share/cityscapesscripts/credentials.json' [y/N]: n\n",
            "Downloading cityscapes package 'leftImg8bit_trainvaltest.zip' to './leftImg8bit_trainvaltest.zip'\n",
            "Download progress:  98% 10.8G/11.0G [09:07<00:10, 21.2MB/s]\n",
            "Cityscapes username or email address: s303369\n",
            "Cityscapes password: \n",
            "Store credentials unencrypted in '/root/.local/share/cityscapesscripts/credentials.json' [y/N]: n\n",
            "Downloading cityscapes package 'gtFine_trainvaltest.zip' to './gtFine_trainvaltest.zip'\n",
            "Download progress: 100% 241M/241M [00:12<00:00, 19.8MB/s]\n",
            "Processing 5000 annotation files\n",
            "Progress: 100.0 % "
          ]
        }
      ],
      "source": [
        "# download cityscapes dataset\n",
        "!pip3 install cityscapesscripts\n",
        "!csDownload leftImg8bit_trainvaltest.zip\n",
        "!csDownload gtFine_trainvaltest.zip\n",
        "!unzip -q 'leftImg8bit_trainvaltest.zip' -d 'cityscapes'\n",
        "!unzip -o -q 'gtFine_trainvaltest.zip' -d 'cityscapes'\n",
        "# generate trainIds from labelIds\n",
        "!CITYSCAPES_DATASET='cityscapes/' csCreateTrainIdLabelImgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scbJ4y5C_nIR",
        "outputId": "4a5e8d49-64b3-4092-d345-a6a354b38435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "# unzip the datasets\n",
        "!unzip -q /content/drive/MyDrive/ProjectAML/Validation_Dataset.zip\n",
        "!mkdir validation_dataset && cp -pR Validation_Dataset/* validation_dataset/ && rm -R Validation_Dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qmj1NP_mAbBA",
        "outputId": "bce5705e-b796-4ad6-891b-e4350b69839b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ood_metrics\n",
            "  Downloading ood_metrics-1.1.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from ood_metrics) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.22 in /usr/local/lib/python3.10/dist-packages (from ood_metrics) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from ood_metrics) (1.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>=3.0->ood_metrics) (1.17.0)\n",
            "Downloading ood_metrics-1.1.2-py3-none-any.whl (6.1 kB)\n",
            "Installing collected packages: ood_metrics\n",
            "Successfully installed ood_metrics-1.1.2\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom) (6.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.10/dist-packages (from visdom) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom) (1.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from visdom) (3.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom) (11.0.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch->visdom) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2024.12.14)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408196 sha256=0c6498185831ed144a5c0c1065f321ebcd1d5b1e725eb86a886878368de44e5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built visdom\n",
            "Installing collected packages: visdom\n",
            "Successfully installed visdom-0.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ood_metrics\n",
        "!pip3 install Pillow\n",
        "!pip3 install torchvision\n",
        "!pip3 install visdom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGUFtDkaofk3"
      },
      "source": [
        "**Model: ERFNet metric: MSP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kz_TBeYNeiE1",
        "outputId": "99d92a65-67b7-44ac-f064-a769cc48614a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/eval\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadObsticle21/images/8.webp\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadObsticle21/images/11.webp\n",
            "../validation_dataset/RoadObsticle21/images/7.webp\n",
            "../validation_dataset/RoadObsticle21/images/17.webp\n",
            "../validation_dataset/RoadObsticle21/images/13.webp\n",
            "../validation_dataset/RoadObsticle21/images/9.webp\n",
            "../validation_dataset/RoadObsticle21/images/10.webp\n",
            "../validation_dataset/RoadObsticle21/images/19.webp\n",
            "../validation_dataset/RoadObsticle21/images/18.webp\n",
            "../validation_dataset/RoadObsticle21/images/27.webp\n",
            "../validation_dataset/RoadObsticle21/images/4.webp\n",
            "../validation_dataset/RoadObsticle21/images/28.webp\n",
            "../validation_dataset/RoadObsticle21/images/14.webp\n",
            "../validation_dataset/RoadObsticle21/images/22.webp\n",
            "../validation_dataset/RoadObsticle21/images/1.webp\n",
            "../validation_dataset/RoadObsticle21/images/12.webp\n",
            "../validation_dataset/RoadObsticle21/images/25.webp\n",
            "../validation_dataset/RoadObsticle21/images/26.webp\n",
            "../validation_dataset/RoadObsticle21/images/3.webp\n",
            "../validation_dataset/RoadObsticle21/images/29.webp\n",
            "../validation_dataset/RoadObsticle21/images/16.webp\n",
            "../validation_dataset/RoadObsticle21/images/0.webp\n",
            "../validation_dataset/RoadObsticle21/images/6.webp\n",
            "../validation_dataset/RoadObsticle21/images/24.webp\n",
            "../validation_dataset/RoadObsticle21/images/5.webp\n",
            "../validation_dataset/RoadObsticle21/images/23.webp\n",
            "../validation_dataset/RoadObsticle21/images/20.webp\n",
            "../validation_dataset/RoadObsticle21/images/21.webp\n",
            "../validation_dataset/RoadObsticle21/images/2.webp\n",
            "../validation_dataset/RoadObsticle21/images/15.webp\n",
            "AUPRC score: 2.7116243119338366\n",
            "FPR@TPR95: 64.9739786894368\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly21/images/2.png\n",
            "../validation_dataset/RoadAnomaly21/images/3.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly21/images/9.png\n",
            "../validation_dataset/RoadAnomaly21/images/7.png\n",
            "../validation_dataset/RoadAnomaly21/images/1.png\n",
            "../validation_dataset/RoadAnomaly21/images/6.png\n",
            "../validation_dataset/RoadAnomaly21/images/5.png\n",
            "../validation_dataset/RoadAnomaly21/images/0.png\n",
            "../validation_dataset/RoadAnomaly21/images/4.png\n",
            "../validation_dataset/RoadAnomaly21/images/8.png\n",
            "AUPRC score: 29.100168300581203\n",
            "FPR@TPR95: 62.51075321069286\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly/images/14.jpg\n",
            "../validation_dataset/RoadAnomaly/images/4.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly/images/30.jpg\n",
            "../validation_dataset/RoadAnomaly/images/49.jpg\n",
            "../validation_dataset/RoadAnomaly/images/40.jpg\n",
            "../validation_dataset/RoadAnomaly/images/50.jpg\n",
            "../validation_dataset/RoadAnomaly/images/12.jpg\n",
            "../validation_dataset/RoadAnomaly/images/15.jpg\n",
            "../validation_dataset/RoadAnomaly/images/27.jpg\n",
            "../validation_dataset/RoadAnomaly/images/10.jpg\n",
            "../validation_dataset/RoadAnomaly/images/23.jpg\n",
            "../validation_dataset/RoadAnomaly/images/22.jpg\n",
            "../validation_dataset/RoadAnomaly/images/39.jpg\n",
            "../validation_dataset/RoadAnomaly/images/18.jpg\n",
            "../validation_dataset/RoadAnomaly/images/58.jpg\n",
            "../validation_dataset/RoadAnomaly/images/19.jpg\n",
            "../validation_dataset/RoadAnomaly/images/51.jpg\n",
            "../validation_dataset/RoadAnomaly/images/1.jpg\n",
            "../validation_dataset/RoadAnomaly/images/36.jpg\n",
            "../validation_dataset/RoadAnomaly/images/35.jpg\n",
            "../validation_dataset/RoadAnomaly/images/42.jpg\n",
            "../validation_dataset/RoadAnomaly/images/53.jpg\n",
            "../validation_dataset/RoadAnomaly/images/44.jpg\n",
            "../validation_dataset/RoadAnomaly/images/33.jpg\n",
            "../validation_dataset/RoadAnomaly/images/46.jpg\n",
            "../validation_dataset/RoadAnomaly/images/0.jpg\n",
            "../validation_dataset/RoadAnomaly/images/3.jpg\n",
            "../validation_dataset/RoadAnomaly/images/55.jpg\n",
            "../validation_dataset/RoadAnomaly/images/24.jpg\n",
            "../validation_dataset/RoadAnomaly/images/11.jpg\n",
            "../validation_dataset/RoadAnomaly/images/43.jpg\n",
            "../validation_dataset/RoadAnomaly/images/34.jpg\n",
            "../validation_dataset/RoadAnomaly/images/5.jpg\n",
            "../validation_dataset/RoadAnomaly/images/41.jpg\n",
            "../validation_dataset/RoadAnomaly/images/56.jpg\n",
            "../validation_dataset/RoadAnomaly/images/54.jpg\n",
            "../validation_dataset/RoadAnomaly/images/37.jpg\n",
            "../validation_dataset/RoadAnomaly/images/29.jpg\n",
            "../validation_dataset/RoadAnomaly/images/8.jpg\n",
            "../validation_dataset/RoadAnomaly/images/2.jpg\n",
            "../validation_dataset/RoadAnomaly/images/25.jpg\n",
            "../validation_dataset/RoadAnomaly/images/28.jpg\n",
            "../validation_dataset/RoadAnomaly/images/7.jpg\n",
            "../validation_dataset/RoadAnomaly/images/13.jpg\n",
            "../validation_dataset/RoadAnomaly/images/45.jpg\n",
            "../validation_dataset/RoadAnomaly/images/38.jpg\n",
            "../validation_dataset/RoadAnomaly/images/20.jpg\n",
            "../validation_dataset/RoadAnomaly/images/47.jpg\n",
            "../validation_dataset/RoadAnomaly/images/17.jpg\n",
            "../validation_dataset/RoadAnomaly/images/57.jpg\n",
            "../validation_dataset/RoadAnomaly/images/9.jpg\n",
            "../validation_dataset/RoadAnomaly/images/48.jpg\n",
            "../validation_dataset/RoadAnomaly/images/26.jpg\n",
            "../validation_dataset/RoadAnomaly/images/16.jpg\n",
            "../validation_dataset/RoadAnomaly/images/21.jpg\n",
            "../validation_dataset/RoadAnomaly/images/6.jpg\n",
            "../validation_dataset/RoadAnomaly/images/31.jpg\n",
            "../validation_dataset/RoadAnomaly/images/52.jpg\n",
            "../validation_dataset/RoadAnomaly/images/59.jpg\n",
            "../validation_dataset/RoadAnomaly/images/32.jpg\n",
            "AUPRC score: 12.426265849563665\n",
            "FPR@TPR95: 82.49244029880458\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/fs_static/images/14.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/fs_static/images/4.jpg\n",
            "../validation_dataset/fs_static/images/12.jpg\n",
            "../validation_dataset/fs_static/images/15.jpg\n",
            "../validation_dataset/fs_static/images/27.jpg\n",
            "../validation_dataset/fs_static/images/10.jpg\n",
            "../validation_dataset/fs_static/images/23.jpg\n",
            "../validation_dataset/fs_static/images/22.jpg\n",
            "../validation_dataset/fs_static/images/18.jpg\n",
            "../validation_dataset/fs_static/images/19.jpg\n",
            "../validation_dataset/fs_static/images/1.jpg\n",
            "../validation_dataset/fs_static/images/0.jpg\n",
            "../validation_dataset/fs_static/images/3.jpg\n",
            "../validation_dataset/fs_static/images/24.jpg\n",
            "../validation_dataset/fs_static/images/11.jpg\n",
            "../validation_dataset/fs_static/images/5.jpg\n",
            "../validation_dataset/fs_static/images/29.jpg\n",
            "../validation_dataset/fs_static/images/8.jpg\n",
            "../validation_dataset/fs_static/images/2.jpg\n",
            "../validation_dataset/fs_static/images/25.jpg\n",
            "../validation_dataset/fs_static/images/28.jpg\n",
            "../validation_dataset/fs_static/images/7.jpg\n",
            "../validation_dataset/fs_static/images/13.jpg\n",
            "../validation_dataset/fs_static/images/20.jpg\n",
            "../validation_dataset/fs_static/images/17.jpg\n",
            "../validation_dataset/fs_static/images/9.jpg\n",
            "../validation_dataset/fs_static/images/26.jpg\n",
            "../validation_dataset/fs_static/images/16.jpg\n",
            "../validation_dataset/fs_static/images/21.jpg\n",
            "../validation_dataset/fs_static/images/6.jpg\n",
            "AUPRC score: 7.4700433549050915\n",
            "FPR@TPR95: 41.82346831776172\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/FS_LostFound_full/images/21.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/FS_LostFound_full/images/46.png\n",
            "../validation_dataset/FS_LostFound_full/images/24.png\n",
            "../validation_dataset/FS_LostFound_full/images/28.png\n",
            "../validation_dataset/FS_LostFound_full/images/32.png\n",
            "../validation_dataset/FS_LostFound_full/images/58.png\n",
            "../validation_dataset/FS_LostFound_full/images/2.png\n",
            "../validation_dataset/FS_LostFound_full/images/31.png\n",
            "../validation_dataset/FS_LostFound_full/images/33.png\n",
            "../validation_dataset/FS_LostFound_full/images/12.png\n",
            "../validation_dataset/FS_LostFound_full/images/94.png\n",
            "../validation_dataset/FS_LostFound_full/images/86.png\n",
            "../validation_dataset/FS_LostFound_full/images/90.png\n",
            "../validation_dataset/FS_LostFound_full/images/47.png\n",
            "../validation_dataset/FS_LostFound_full/images/93.png\n",
            "../validation_dataset/FS_LostFound_full/images/27.png\n",
            "../validation_dataset/FS_LostFound_full/images/68.png\n",
            "../validation_dataset/FS_LostFound_full/images/97.png\n",
            "../validation_dataset/FS_LostFound_full/images/62.png\n",
            "../validation_dataset/FS_LostFound_full/images/85.png\n",
            "../validation_dataset/FS_LostFound_full/images/38.png\n",
            "../validation_dataset/FS_LostFound_full/images/91.png\n",
            "../validation_dataset/FS_LostFound_full/images/74.png\n",
            "../validation_dataset/FS_LostFound_full/images/66.png\n",
            "../validation_dataset/FS_LostFound_full/images/37.png\n",
            "../validation_dataset/FS_LostFound_full/images/30.png\n",
            "../validation_dataset/FS_LostFound_full/images/3.png\n",
            "../validation_dataset/FS_LostFound_full/images/29.png\n",
            "../validation_dataset/FS_LostFound_full/images/87.png\n",
            "../validation_dataset/FS_LostFound_full/images/13.png\n",
            "../validation_dataset/FS_LostFound_full/images/14.png\n",
            "../validation_dataset/FS_LostFound_full/images/92.png\n",
            "../validation_dataset/FS_LostFound_full/images/64.png\n",
            "../validation_dataset/FS_LostFound_full/images/67.png\n",
            "../validation_dataset/FS_LostFound_full/images/22.png\n",
            "../validation_dataset/FS_LostFound_full/images/72.png\n",
            "../validation_dataset/FS_LostFound_full/images/65.png\n",
            "../validation_dataset/FS_LostFound_full/images/81.png\n",
            "../validation_dataset/FS_LostFound_full/images/34.png\n",
            "../validation_dataset/FS_LostFound_full/images/20.png\n",
            "../validation_dataset/FS_LostFound_full/images/16.png\n",
            "../validation_dataset/FS_LostFound_full/images/43.png\n",
            "../validation_dataset/FS_LostFound_full/images/17.png\n",
            "../validation_dataset/FS_LostFound_full/images/88.png\n",
            "../validation_dataset/FS_LostFound_full/images/9.png\n",
            "../validation_dataset/FS_LostFound_full/images/42.png\n",
            "../validation_dataset/FS_LostFound_full/images/7.png\n",
            "../validation_dataset/FS_LostFound_full/images/1.png\n",
            "../validation_dataset/FS_LostFound_full/images/6.png\n",
            "../validation_dataset/FS_LostFound_full/images/55.png\n",
            "../validation_dataset/FS_LostFound_full/images/51.png\n",
            "../validation_dataset/FS_LostFound_full/images/10.png\n",
            "../validation_dataset/FS_LostFound_full/images/36.png\n",
            "../validation_dataset/FS_LostFound_full/images/95.png\n",
            "../validation_dataset/FS_LostFound_full/images/98.png\n",
            "../validation_dataset/FS_LostFound_full/images/35.png\n",
            "../validation_dataset/FS_LostFound_full/images/76.png\n",
            "../validation_dataset/FS_LostFound_full/images/78.png\n",
            "../validation_dataset/FS_LostFound_full/images/52.png\n",
            "../validation_dataset/FS_LostFound_full/images/45.png\n",
            "../validation_dataset/FS_LostFound_full/images/54.png\n",
            "../validation_dataset/FS_LostFound_full/images/83.png\n",
            "../validation_dataset/FS_LostFound_full/images/5.png\n",
            "../validation_dataset/FS_LostFound_full/images/79.png\n",
            "../validation_dataset/FS_LostFound_full/images/96.png\n",
            "../validation_dataset/FS_LostFound_full/images/71.png\n",
            "../validation_dataset/FS_LostFound_full/images/89.png\n",
            "../validation_dataset/FS_LostFound_full/images/18.png\n",
            "../validation_dataset/FS_LostFound_full/images/57.png\n",
            "../validation_dataset/FS_LostFound_full/images/25.png\n",
            "../validation_dataset/FS_LostFound_full/images/19.png\n",
            "../validation_dataset/FS_LostFound_full/images/63.png\n",
            "../validation_dataset/FS_LostFound_full/images/70.png\n",
            "../validation_dataset/FS_LostFound_full/images/39.png\n",
            "../validation_dataset/FS_LostFound_full/images/56.png\n",
            "../validation_dataset/FS_LostFound_full/images/0.png\n",
            "../validation_dataset/FS_LostFound_full/images/49.png\n",
            "../validation_dataset/FS_LostFound_full/images/4.png\n",
            "../validation_dataset/FS_LostFound_full/images/8.png\n",
            "../validation_dataset/FS_LostFound_full/images/99.png\n",
            "../validation_dataset/FS_LostFound_full/images/61.png\n",
            "../validation_dataset/FS_LostFound_full/images/50.png\n",
            "../validation_dataset/FS_LostFound_full/images/82.png\n",
            "../validation_dataset/FS_LostFound_full/images/80.png\n",
            "../validation_dataset/FS_LostFound_full/images/11.png\n",
            "../validation_dataset/FS_LostFound_full/images/73.png\n",
            "../validation_dataset/FS_LostFound_full/images/60.png\n",
            "../validation_dataset/FS_LostFound_full/images/41.png\n",
            "../validation_dataset/FS_LostFound_full/images/84.png\n",
            "../validation_dataset/FS_LostFound_full/images/53.png\n",
            "../validation_dataset/FS_LostFound_full/images/75.png\n",
            "../validation_dataset/FS_LostFound_full/images/77.png\n",
            "../validation_dataset/FS_LostFound_full/images/59.png\n",
            "../validation_dataset/FS_LostFound_full/images/44.png\n",
            "../validation_dataset/FS_LostFound_full/images/15.png\n",
            "../validation_dataset/FS_LostFound_full/images/69.png\n",
            "../validation_dataset/FS_LostFound_full/images/23.png\n",
            "../validation_dataset/FS_LostFound_full/images/26.png\n",
            "../validation_dataset/FS_LostFound_full/images/48.png\n",
            "../validation_dataset/FS_LostFound_full/images/40.png\n",
            "AUPRC score: 1.747872547607269\n",
            "FPR@TPR95: 50.76348570192957\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd eval\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadObsticle21/images/*.webp\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly21/images/*.png\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly/images/*.jpg\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/fs_static/images/*.jpg\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/FS_LostFound_full/images/*.png\"\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyXD5yORo_7S"
      },
      "source": [
        "**Model: ERFNet metric: MaxLogit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R9AqgVrJonoV",
        "outputId": "ffcf10e6-36f1-44ae-9741-f94f57d8068a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/eval\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadObsticle21/images/8.webp\n",
            "../validation_dataset/RoadObsticle21/images/11.webp\n",
            "../validation_dataset/RoadObsticle21/images/7.webp\n",
            "../validation_dataset/RoadObsticle21/images/17.webp\n",
            "../validation_dataset/RoadObsticle21/images/13.webp\n",
            "../validation_dataset/RoadObsticle21/images/9.webp\n",
            "../validation_dataset/RoadObsticle21/images/10.webp\n",
            "../validation_dataset/RoadObsticle21/images/19.webp\n",
            "../validation_dataset/RoadObsticle21/images/18.webp\n",
            "../validation_dataset/RoadObsticle21/images/27.webp\n",
            "../validation_dataset/RoadObsticle21/images/4.webp\n",
            "../validation_dataset/RoadObsticle21/images/28.webp\n",
            "../validation_dataset/RoadObsticle21/images/14.webp\n",
            "../validation_dataset/RoadObsticle21/images/22.webp\n",
            "../validation_dataset/RoadObsticle21/images/1.webp\n",
            "../validation_dataset/RoadObsticle21/images/12.webp\n",
            "../validation_dataset/RoadObsticle21/images/25.webp\n",
            "../validation_dataset/RoadObsticle21/images/26.webp\n",
            "../validation_dataset/RoadObsticle21/images/3.webp\n",
            "../validation_dataset/RoadObsticle21/images/29.webp\n",
            "../validation_dataset/RoadObsticle21/images/16.webp\n",
            "../validation_dataset/RoadObsticle21/images/0.webp\n",
            "../validation_dataset/RoadObsticle21/images/6.webp\n",
            "../validation_dataset/RoadObsticle21/images/24.webp\n",
            "../validation_dataset/RoadObsticle21/images/5.webp\n",
            "../validation_dataset/RoadObsticle21/images/23.webp\n",
            "../validation_dataset/RoadObsticle21/images/20.webp\n",
            "../validation_dataset/RoadObsticle21/images/21.webp\n",
            "../validation_dataset/RoadObsticle21/images/2.webp\n",
            "../validation_dataset/RoadObsticle21/images/15.webp\n",
            "Metric: maxLogit\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly21/images/2.png\n",
            "../validation_dataset/RoadAnomaly21/images/3.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly21/images/9.png\n",
            "../validation_dataset/RoadAnomaly21/images/7.png\n",
            "../validation_dataset/RoadAnomaly21/images/1.png\n",
            "../validation_dataset/RoadAnomaly21/images/6.png\n",
            "../validation_dataset/RoadAnomaly21/images/5.png\n",
            "../validation_dataset/RoadAnomaly21/images/0.png\n",
            "../validation_dataset/RoadAnomaly21/images/4.png\n",
            "../validation_dataset/RoadAnomaly21/images/8.png\n",
            "Metric: maxLogit\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly/images/14.jpg\n",
            "../validation_dataset/RoadAnomaly/images/4.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly/images/30.jpg\n",
            "../validation_dataset/RoadAnomaly/images/49.jpg\n",
            "../validation_dataset/RoadAnomaly/images/40.jpg\n",
            "../validation_dataset/RoadAnomaly/images/50.jpg\n",
            "../validation_dataset/RoadAnomaly/images/12.jpg\n",
            "../validation_dataset/RoadAnomaly/images/15.jpg\n",
            "../validation_dataset/RoadAnomaly/images/27.jpg\n",
            "../validation_dataset/RoadAnomaly/images/10.jpg\n",
            "../validation_dataset/RoadAnomaly/images/23.jpg\n",
            "../validation_dataset/RoadAnomaly/images/22.jpg\n",
            "../validation_dataset/RoadAnomaly/images/39.jpg\n",
            "../validation_dataset/RoadAnomaly/images/18.jpg\n",
            "../validation_dataset/RoadAnomaly/images/58.jpg\n",
            "../validation_dataset/RoadAnomaly/images/19.jpg\n",
            "../validation_dataset/RoadAnomaly/images/51.jpg\n",
            "../validation_dataset/RoadAnomaly/images/1.jpg\n",
            "../validation_dataset/RoadAnomaly/images/36.jpg\n",
            "../validation_dataset/RoadAnomaly/images/35.jpg\n",
            "../validation_dataset/RoadAnomaly/images/42.jpg\n",
            "../validation_dataset/RoadAnomaly/images/53.jpg\n",
            "../validation_dataset/RoadAnomaly/images/44.jpg\n",
            "../validation_dataset/RoadAnomaly/images/33.jpg\n",
            "../validation_dataset/RoadAnomaly/images/46.jpg\n",
            "../validation_dataset/RoadAnomaly/images/0.jpg\n",
            "../validation_dataset/RoadAnomaly/images/3.jpg\n",
            "../validation_dataset/RoadAnomaly/images/55.jpg\n",
            "../validation_dataset/RoadAnomaly/images/24.jpg\n",
            "../validation_dataset/RoadAnomaly/images/11.jpg\n",
            "../validation_dataset/RoadAnomaly/images/43.jpg\n",
            "../validation_dataset/RoadAnomaly/images/34.jpg\n",
            "../validation_dataset/RoadAnomaly/images/5.jpg\n",
            "../validation_dataset/RoadAnomaly/images/41.jpg\n",
            "../validation_dataset/RoadAnomaly/images/56.jpg\n",
            "../validation_dataset/RoadAnomaly/images/54.jpg\n",
            "../validation_dataset/RoadAnomaly/images/37.jpg\n",
            "../validation_dataset/RoadAnomaly/images/29.jpg\n",
            "../validation_dataset/RoadAnomaly/images/8.jpg\n",
            "../validation_dataset/RoadAnomaly/images/2.jpg\n",
            "../validation_dataset/RoadAnomaly/images/25.jpg\n",
            "../validation_dataset/RoadAnomaly/images/28.jpg\n",
            "../validation_dataset/RoadAnomaly/images/7.jpg\n",
            "../validation_dataset/RoadAnomaly/images/13.jpg\n",
            "../validation_dataset/RoadAnomaly/images/45.jpg\n",
            "../validation_dataset/RoadAnomaly/images/38.jpg\n",
            "../validation_dataset/RoadAnomaly/images/20.jpg\n",
            "../validation_dataset/RoadAnomaly/images/47.jpg\n",
            "../validation_dataset/RoadAnomaly/images/17.jpg\n",
            "../validation_dataset/RoadAnomaly/images/57.jpg\n",
            "../validation_dataset/RoadAnomaly/images/9.jpg\n",
            "../validation_dataset/RoadAnomaly/images/48.jpg\n",
            "../validation_dataset/RoadAnomaly/images/26.jpg\n",
            "../validation_dataset/RoadAnomaly/images/16.jpg\n",
            "../validation_dataset/RoadAnomaly/images/21.jpg\n",
            "../validation_dataset/RoadAnomaly/images/6.jpg\n",
            "../validation_dataset/RoadAnomaly/images/31.jpg\n",
            "../validation_dataset/RoadAnomaly/images/52.jpg\n",
            "../validation_dataset/RoadAnomaly/images/59.jpg\n",
            "../validation_dataset/RoadAnomaly/images/32.jpg\n",
            "Metric: maxLogit\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/fs_static/images/14.jpg\n",
            "../validation_dataset/fs_static/images/4.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/fs_static/images/12.jpg\n",
            "../validation_dataset/fs_static/images/15.jpg\n",
            "../validation_dataset/fs_static/images/27.jpg\n",
            "../validation_dataset/fs_static/images/10.jpg\n",
            "../validation_dataset/fs_static/images/23.jpg\n",
            "../validation_dataset/fs_static/images/22.jpg\n",
            "../validation_dataset/fs_static/images/18.jpg\n",
            "../validation_dataset/fs_static/images/19.jpg\n",
            "../validation_dataset/fs_static/images/1.jpg\n",
            "../validation_dataset/fs_static/images/0.jpg\n",
            "../validation_dataset/fs_static/images/3.jpg\n",
            "../validation_dataset/fs_static/images/24.jpg\n",
            "../validation_dataset/fs_static/images/11.jpg\n",
            "../validation_dataset/fs_static/images/5.jpg\n",
            "../validation_dataset/fs_static/images/29.jpg\n",
            "../validation_dataset/fs_static/images/8.jpg\n",
            "../validation_dataset/fs_static/images/2.jpg\n",
            "../validation_dataset/fs_static/images/25.jpg\n",
            "../validation_dataset/fs_static/images/28.jpg\n",
            "../validation_dataset/fs_static/images/7.jpg\n",
            "../validation_dataset/fs_static/images/13.jpg\n",
            "../validation_dataset/fs_static/images/20.jpg\n",
            "../validation_dataset/fs_static/images/17.jpg\n",
            "../validation_dataset/fs_static/images/9.jpg\n",
            "../validation_dataset/fs_static/images/26.jpg\n",
            "../validation_dataset/fs_static/images/16.jpg\n",
            "../validation_dataset/fs_static/images/21.jpg\n",
            "../validation_dataset/fs_static/images/6.jpg\n",
            "Metric: maxLogit\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/FS_LostFound_full/images/21.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/FS_LostFound_full/images/46.png\n",
            "../validation_dataset/FS_LostFound_full/images/24.png\n",
            "../validation_dataset/FS_LostFound_full/images/28.png\n",
            "../validation_dataset/FS_LostFound_full/images/32.png\n",
            "../validation_dataset/FS_LostFound_full/images/58.png\n",
            "../validation_dataset/FS_LostFound_full/images/2.png\n",
            "../validation_dataset/FS_LostFound_full/images/31.png\n",
            "../validation_dataset/FS_LostFound_full/images/33.png\n",
            "../validation_dataset/FS_LostFound_full/images/12.png\n",
            "../validation_dataset/FS_LostFound_full/images/94.png\n",
            "../validation_dataset/FS_LostFound_full/images/86.png\n",
            "../validation_dataset/FS_LostFound_full/images/90.png\n",
            "../validation_dataset/FS_LostFound_full/images/47.png\n",
            "../validation_dataset/FS_LostFound_full/images/93.png\n",
            "../validation_dataset/FS_LostFound_full/images/27.png\n",
            "../validation_dataset/FS_LostFound_full/images/68.png\n",
            "../validation_dataset/FS_LostFound_full/images/97.png\n",
            "../validation_dataset/FS_LostFound_full/images/62.png\n",
            "../validation_dataset/FS_LostFound_full/images/85.png\n",
            "../validation_dataset/FS_LostFound_full/images/38.png\n",
            "../validation_dataset/FS_LostFound_full/images/91.png\n",
            "../validation_dataset/FS_LostFound_full/images/74.png\n",
            "../validation_dataset/FS_LostFound_full/images/66.png\n",
            "../validation_dataset/FS_LostFound_full/images/37.png\n",
            "../validation_dataset/FS_LostFound_full/images/30.png\n",
            "../validation_dataset/FS_LostFound_full/images/3.png\n",
            "../validation_dataset/FS_LostFound_full/images/29.png\n",
            "../validation_dataset/FS_LostFound_full/images/87.png\n",
            "../validation_dataset/FS_LostFound_full/images/13.png\n",
            "../validation_dataset/FS_LostFound_full/images/14.png\n",
            "../validation_dataset/FS_LostFound_full/images/92.png\n",
            "../validation_dataset/FS_LostFound_full/images/64.png\n",
            "../validation_dataset/FS_LostFound_full/images/67.png\n",
            "../validation_dataset/FS_LostFound_full/images/22.png\n",
            "../validation_dataset/FS_LostFound_full/images/72.png\n",
            "../validation_dataset/FS_LostFound_full/images/65.png\n",
            "../validation_dataset/FS_LostFound_full/images/81.png\n",
            "../validation_dataset/FS_LostFound_full/images/34.png\n",
            "../validation_dataset/FS_LostFound_full/images/20.png\n",
            "../validation_dataset/FS_LostFound_full/images/16.png\n",
            "../validation_dataset/FS_LostFound_full/images/43.png\n",
            "../validation_dataset/FS_LostFound_full/images/17.png\n",
            "../validation_dataset/FS_LostFound_full/images/88.png\n",
            "../validation_dataset/FS_LostFound_full/images/9.png\n",
            "../validation_dataset/FS_LostFound_full/images/42.png\n",
            "../validation_dataset/FS_LostFound_full/images/7.png\n",
            "../validation_dataset/FS_LostFound_full/images/1.png\n",
            "../validation_dataset/FS_LostFound_full/images/6.png\n",
            "../validation_dataset/FS_LostFound_full/images/55.png\n",
            "../validation_dataset/FS_LostFound_full/images/51.png\n",
            "../validation_dataset/FS_LostFound_full/images/10.png\n",
            "../validation_dataset/FS_LostFound_full/images/36.png\n",
            "../validation_dataset/FS_LostFound_full/images/95.png\n",
            "../validation_dataset/FS_LostFound_full/images/98.png\n",
            "../validation_dataset/FS_LostFound_full/images/35.png\n",
            "../validation_dataset/FS_LostFound_full/images/76.png\n",
            "../validation_dataset/FS_LostFound_full/images/78.png\n",
            "../validation_dataset/FS_LostFound_full/images/52.png\n",
            "../validation_dataset/FS_LostFound_full/images/45.png\n",
            "../validation_dataset/FS_LostFound_full/images/54.png\n",
            "../validation_dataset/FS_LostFound_full/images/83.png\n",
            "../validation_dataset/FS_LostFound_full/images/5.png\n",
            "../validation_dataset/FS_LostFound_full/images/79.png\n",
            "../validation_dataset/FS_LostFound_full/images/96.png\n",
            "../validation_dataset/FS_LostFound_full/images/71.png\n",
            "../validation_dataset/FS_LostFound_full/images/89.png\n",
            "../validation_dataset/FS_LostFound_full/images/18.png\n",
            "../validation_dataset/FS_LostFound_full/images/57.png\n",
            "../validation_dataset/FS_LostFound_full/images/25.png\n",
            "../validation_dataset/FS_LostFound_full/images/19.png\n",
            "../validation_dataset/FS_LostFound_full/images/63.png\n",
            "../validation_dataset/FS_LostFound_full/images/70.png\n",
            "../validation_dataset/FS_LostFound_full/images/39.png\n",
            "../validation_dataset/FS_LostFound_full/images/56.png\n",
            "../validation_dataset/FS_LostFound_full/images/0.png\n",
            "../validation_dataset/FS_LostFound_full/images/49.png\n",
            "../validation_dataset/FS_LostFound_full/images/4.png\n",
            "../validation_dataset/FS_LostFound_full/images/8.png\n",
            "../validation_dataset/FS_LostFound_full/images/99.png\n",
            "../validation_dataset/FS_LostFound_full/images/61.png\n",
            "../validation_dataset/FS_LostFound_full/images/50.png\n",
            "../validation_dataset/FS_LostFound_full/images/82.png\n",
            "../validation_dataset/FS_LostFound_full/images/80.png\n",
            "../validation_dataset/FS_LostFound_full/images/11.png\n",
            "../validation_dataset/FS_LostFound_full/images/73.png\n",
            "../validation_dataset/FS_LostFound_full/images/60.png\n",
            "../validation_dataset/FS_LostFound_full/images/41.png\n",
            "../validation_dataset/FS_LostFound_full/images/84.png\n",
            "../validation_dataset/FS_LostFound_full/images/53.png\n",
            "../validation_dataset/FS_LostFound_full/images/75.png\n",
            "../validation_dataset/FS_LostFound_full/images/77.png\n",
            "../validation_dataset/FS_LostFound_full/images/59.png\n",
            "../validation_dataset/FS_LostFound_full/images/44.png\n",
            "../validation_dataset/FS_LostFound_full/images/15.png\n",
            "../validation_dataset/FS_LostFound_full/images/69.png\n",
            "../validation_dataset/FS_LostFound_full/images/23.png\n",
            "../validation_dataset/FS_LostFound_full/images/26.png\n",
            "../validation_dataset/FS_LostFound_full/images/48.png\n",
            "../validation_dataset/FS_LostFound_full/images/40.png\n",
            "Metric: maxLogit\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd eval\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadObsticle21/images/*.webp\" --metric=\"maxLogit\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly21/images/*.png\" --metric=\"maxLogit\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly/images/*.jpg\" --metric=\"maxLogit\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/fs_static/images/*.jpg\" --metric=\"maxLogit\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/FS_LostFound_full/images/*.png\" --metric=\"maxLogit\"\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiXJRP0ApWtQ"
      },
      "source": [
        "**Model: ERFNet metric: MaxEntropy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDzBSKQFpfWd",
        "outputId": "3ae0af62-f812-419f-edaf-8d16020a3f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/eval\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadObsticle21/images/8.webp\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadObsticle21/images/11.webp\n",
            "../validation_dataset/RoadObsticle21/images/7.webp\n",
            "../validation_dataset/RoadObsticle21/images/17.webp\n",
            "../validation_dataset/RoadObsticle21/images/13.webp\n",
            "../validation_dataset/RoadObsticle21/images/9.webp\n",
            "../validation_dataset/RoadObsticle21/images/10.webp\n",
            "../validation_dataset/RoadObsticle21/images/19.webp\n",
            "../validation_dataset/RoadObsticle21/images/18.webp\n",
            "../validation_dataset/RoadObsticle21/images/27.webp\n",
            "../validation_dataset/RoadObsticle21/images/4.webp\n",
            "../validation_dataset/RoadObsticle21/images/28.webp\n",
            "../validation_dataset/RoadObsticle21/images/14.webp\n",
            "../validation_dataset/RoadObsticle21/images/22.webp\n",
            "../validation_dataset/RoadObsticle21/images/1.webp\n",
            "../validation_dataset/RoadObsticle21/images/12.webp\n",
            "../validation_dataset/RoadObsticle21/images/25.webp\n",
            "../validation_dataset/RoadObsticle21/images/26.webp\n",
            "../validation_dataset/RoadObsticle21/images/3.webp\n",
            "../validation_dataset/RoadObsticle21/images/29.webp\n",
            "../validation_dataset/RoadObsticle21/images/16.webp\n",
            "../validation_dataset/RoadObsticle21/images/0.webp\n",
            "../validation_dataset/RoadObsticle21/images/6.webp\n",
            "../validation_dataset/RoadObsticle21/images/24.webp\n",
            "../validation_dataset/RoadObsticle21/images/5.webp\n",
            "../validation_dataset/RoadObsticle21/images/23.webp\n",
            "../validation_dataset/RoadObsticle21/images/20.webp\n",
            "../validation_dataset/RoadObsticle21/images/21.webp\n",
            "../validation_dataset/RoadObsticle21/images/2.webp\n",
            "../validation_dataset/RoadObsticle21/images/15.webp\n",
            "Metric: maxEntropy\n",
            "AUPRC score: 3.0515600030082872\n",
            "FPR@TPR95: 65.59951691861312\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly21/images/2.png\n",
            "../validation_dataset/RoadAnomaly21/images/3.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly21/images/9.png\n",
            "../validation_dataset/RoadAnomaly21/images/7.png\n",
            "../validation_dataset/RoadAnomaly21/images/1.png\n",
            "../validation_dataset/RoadAnomaly21/images/6.png\n",
            "../validation_dataset/RoadAnomaly21/images/5.png\n",
            "../validation_dataset/RoadAnomaly21/images/0.png\n",
            "../validation_dataset/RoadAnomaly21/images/4.png\n",
            "../validation_dataset/RoadAnomaly21/images/8.png\n",
            "Metric: maxEntropy\n",
            "AUPRC score: 31.00510240772473\n",
            "FPR@TPR95: 62.5931151771679\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly/images/14.jpg\n",
            "../validation_dataset/RoadAnomaly/images/4.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly/images/30.jpg\n",
            "../validation_dataset/RoadAnomaly/images/49.jpg\n",
            "../validation_dataset/RoadAnomaly/images/40.jpg\n",
            "../validation_dataset/RoadAnomaly/images/50.jpg\n",
            "../validation_dataset/RoadAnomaly/images/12.jpg\n",
            "../validation_dataset/RoadAnomaly/images/15.jpg\n",
            "../validation_dataset/RoadAnomaly/images/27.jpg\n",
            "../validation_dataset/RoadAnomaly/images/10.jpg\n",
            "../validation_dataset/RoadAnomaly/images/23.jpg\n",
            "../validation_dataset/RoadAnomaly/images/22.jpg\n",
            "../validation_dataset/RoadAnomaly/images/39.jpg\n",
            "../validation_dataset/RoadAnomaly/images/18.jpg\n",
            "../validation_dataset/RoadAnomaly/images/58.jpg\n",
            "../validation_dataset/RoadAnomaly/images/19.jpg\n",
            "../validation_dataset/RoadAnomaly/images/51.jpg\n",
            "../validation_dataset/RoadAnomaly/images/1.jpg\n",
            "../validation_dataset/RoadAnomaly/images/36.jpg\n",
            "../validation_dataset/RoadAnomaly/images/35.jpg\n",
            "../validation_dataset/RoadAnomaly/images/42.jpg\n",
            "../validation_dataset/RoadAnomaly/images/53.jpg\n",
            "../validation_dataset/RoadAnomaly/images/44.jpg\n",
            "../validation_dataset/RoadAnomaly/images/33.jpg\n",
            "../validation_dataset/RoadAnomaly/images/46.jpg\n",
            "../validation_dataset/RoadAnomaly/images/0.jpg\n",
            "../validation_dataset/RoadAnomaly/images/3.jpg\n",
            "../validation_dataset/RoadAnomaly/images/55.jpg\n",
            "../validation_dataset/RoadAnomaly/images/24.jpg\n",
            "../validation_dataset/RoadAnomaly/images/11.jpg\n",
            "../validation_dataset/RoadAnomaly/images/43.jpg\n",
            "../validation_dataset/RoadAnomaly/images/34.jpg\n",
            "../validation_dataset/RoadAnomaly/images/5.jpg\n",
            "../validation_dataset/RoadAnomaly/images/41.jpg\n",
            "../validation_dataset/RoadAnomaly/images/56.jpg\n",
            "../validation_dataset/RoadAnomaly/images/54.jpg\n",
            "../validation_dataset/RoadAnomaly/images/37.jpg\n",
            "../validation_dataset/RoadAnomaly/images/29.jpg\n",
            "../validation_dataset/RoadAnomaly/images/8.jpg\n",
            "../validation_dataset/RoadAnomaly/images/2.jpg\n",
            "../validation_dataset/RoadAnomaly/images/25.jpg\n",
            "../validation_dataset/RoadAnomaly/images/28.jpg\n",
            "../validation_dataset/RoadAnomaly/images/7.jpg\n",
            "../validation_dataset/RoadAnomaly/images/13.jpg\n",
            "../validation_dataset/RoadAnomaly/images/45.jpg\n",
            "../validation_dataset/RoadAnomaly/images/38.jpg\n",
            "../validation_dataset/RoadAnomaly/images/20.jpg\n",
            "../validation_dataset/RoadAnomaly/images/47.jpg\n",
            "../validation_dataset/RoadAnomaly/images/17.jpg\n",
            "../validation_dataset/RoadAnomaly/images/57.jpg\n",
            "../validation_dataset/RoadAnomaly/images/9.jpg\n",
            "../validation_dataset/RoadAnomaly/images/48.jpg\n",
            "../validation_dataset/RoadAnomaly/images/26.jpg\n",
            "../validation_dataset/RoadAnomaly/images/16.jpg\n",
            "../validation_dataset/RoadAnomaly/images/21.jpg\n",
            "../validation_dataset/RoadAnomaly/images/6.jpg\n",
            "../validation_dataset/RoadAnomaly/images/31.jpg\n",
            "../validation_dataset/RoadAnomaly/images/52.jpg\n",
            "../validation_dataset/RoadAnomaly/images/59.jpg\n",
            "../validation_dataset/RoadAnomaly/images/32.jpg\n",
            "Metric: maxEntropy\n",
            "AUPRC score: 12.678035095375265\n",
            "FPR@TPR95: 82.63187938168642\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/fs_static/images/14.jpg\n",
            "../validation_dataset/fs_static/images/4.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/fs_static/images/12.jpg\n",
            "../validation_dataset/fs_static/images/15.jpg\n",
            "../validation_dataset/fs_static/images/27.jpg\n",
            "../validation_dataset/fs_static/images/10.jpg\n",
            "../validation_dataset/fs_static/images/23.jpg\n",
            "../validation_dataset/fs_static/images/22.jpg\n",
            "../validation_dataset/fs_static/images/18.jpg\n",
            "../validation_dataset/fs_static/images/19.jpg\n",
            "../validation_dataset/fs_static/images/1.jpg\n",
            "../validation_dataset/fs_static/images/0.jpg\n",
            "../validation_dataset/fs_static/images/3.jpg\n",
            "../validation_dataset/fs_static/images/24.jpg\n",
            "../validation_dataset/fs_static/images/11.jpg\n",
            "../validation_dataset/fs_static/images/5.jpg\n",
            "../validation_dataset/fs_static/images/29.jpg\n",
            "../validation_dataset/fs_static/images/8.jpg\n",
            "../validation_dataset/fs_static/images/2.jpg\n",
            "../validation_dataset/fs_static/images/25.jpg\n",
            "../validation_dataset/fs_static/images/28.jpg\n",
            "../validation_dataset/fs_static/images/7.jpg\n",
            "../validation_dataset/fs_static/images/13.jpg\n",
            "../validation_dataset/fs_static/images/20.jpg\n",
            "../validation_dataset/fs_static/images/17.jpg\n",
            "../validation_dataset/fs_static/images/9.jpg\n",
            "../validation_dataset/fs_static/images/26.jpg\n",
            "../validation_dataset/fs_static/images/16.jpg\n",
            "../validation_dataset/fs_static/images/21.jpg\n",
            "../validation_dataset/fs_static/images/6.jpg\n",
            "Metric: maxEntropy\n",
            "AUPRC score: 8.826366102284831\n",
            "FPR@TPR95: 41.52334997616199\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/FS_LostFound_full/images/21.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/FS_LostFound_full/images/46.png\n",
            "../validation_dataset/FS_LostFound_full/images/24.png\n",
            "../validation_dataset/FS_LostFound_full/images/28.png\n",
            "../validation_dataset/FS_LostFound_full/images/32.png\n",
            "../validation_dataset/FS_LostFound_full/images/58.png\n",
            "../validation_dataset/FS_LostFound_full/images/2.png\n",
            "../validation_dataset/FS_LostFound_full/images/31.png\n",
            "../validation_dataset/FS_LostFound_full/images/33.png\n",
            "../validation_dataset/FS_LostFound_full/images/12.png\n",
            "../validation_dataset/FS_LostFound_full/images/94.png\n",
            "../validation_dataset/FS_LostFound_full/images/86.png\n",
            "../validation_dataset/FS_LostFound_full/images/90.png\n",
            "../validation_dataset/FS_LostFound_full/images/47.png\n",
            "../validation_dataset/FS_LostFound_full/images/93.png\n",
            "../validation_dataset/FS_LostFound_full/images/27.png\n",
            "../validation_dataset/FS_LostFound_full/images/68.png\n",
            "../validation_dataset/FS_LostFound_full/images/97.png\n",
            "../validation_dataset/FS_LostFound_full/images/62.png\n",
            "../validation_dataset/FS_LostFound_full/images/85.png\n",
            "../validation_dataset/FS_LostFound_full/images/38.png\n",
            "../validation_dataset/FS_LostFound_full/images/91.png\n",
            "../validation_dataset/FS_LostFound_full/images/74.png\n",
            "../validation_dataset/FS_LostFound_full/images/66.png\n",
            "../validation_dataset/FS_LostFound_full/images/37.png\n",
            "../validation_dataset/FS_LostFound_full/images/30.png\n",
            "../validation_dataset/FS_LostFound_full/images/3.png\n",
            "../validation_dataset/FS_LostFound_full/images/29.png\n",
            "../validation_dataset/FS_LostFound_full/images/87.png\n",
            "../validation_dataset/FS_LostFound_full/images/13.png\n",
            "../validation_dataset/FS_LostFound_full/images/14.png\n",
            "../validation_dataset/FS_LostFound_full/images/92.png\n",
            "../validation_dataset/FS_LostFound_full/images/64.png\n",
            "../validation_dataset/FS_LostFound_full/images/67.png\n",
            "../validation_dataset/FS_LostFound_full/images/22.png\n",
            "../validation_dataset/FS_LostFound_full/images/72.png\n",
            "../validation_dataset/FS_LostFound_full/images/65.png\n",
            "../validation_dataset/FS_LostFound_full/images/81.png\n",
            "../validation_dataset/FS_LostFound_full/images/34.png\n",
            "../validation_dataset/FS_LostFound_full/images/20.png\n",
            "../validation_dataset/FS_LostFound_full/images/16.png\n",
            "../validation_dataset/FS_LostFound_full/images/43.png\n",
            "../validation_dataset/FS_LostFound_full/images/17.png\n",
            "../validation_dataset/FS_LostFound_full/images/88.png\n",
            "../validation_dataset/FS_LostFound_full/images/9.png\n",
            "../validation_dataset/FS_LostFound_full/images/42.png\n",
            "../validation_dataset/FS_LostFound_full/images/7.png\n",
            "../validation_dataset/FS_LostFound_full/images/1.png\n",
            "../validation_dataset/FS_LostFound_full/images/6.png\n",
            "../validation_dataset/FS_LostFound_full/images/55.png\n",
            "../validation_dataset/FS_LostFound_full/images/51.png\n",
            "../validation_dataset/FS_LostFound_full/images/10.png\n",
            "../validation_dataset/FS_LostFound_full/images/36.png\n",
            "../validation_dataset/FS_LostFound_full/images/95.png\n",
            "../validation_dataset/FS_LostFound_full/images/98.png\n",
            "../validation_dataset/FS_LostFound_full/images/35.png\n",
            "../validation_dataset/FS_LostFound_full/images/76.png\n",
            "../validation_dataset/FS_LostFound_full/images/78.png\n",
            "../validation_dataset/FS_LostFound_full/images/52.png\n",
            "../validation_dataset/FS_LostFound_full/images/45.png\n",
            "../validation_dataset/FS_LostFound_full/images/54.png\n",
            "../validation_dataset/FS_LostFound_full/images/83.png\n",
            "../validation_dataset/FS_LostFound_full/images/5.png\n",
            "../validation_dataset/FS_LostFound_full/images/79.png\n",
            "../validation_dataset/FS_LostFound_full/images/96.png\n",
            "../validation_dataset/FS_LostFound_full/images/71.png\n",
            "../validation_dataset/FS_LostFound_full/images/89.png\n",
            "../validation_dataset/FS_LostFound_full/images/18.png\n",
            "../validation_dataset/FS_LostFound_full/images/57.png\n",
            "../validation_dataset/FS_LostFound_full/images/25.png\n",
            "../validation_dataset/FS_LostFound_full/images/19.png\n",
            "../validation_dataset/FS_LostFound_full/images/63.png\n",
            "../validation_dataset/FS_LostFound_full/images/70.png\n",
            "../validation_dataset/FS_LostFound_full/images/39.png\n",
            "../validation_dataset/FS_LostFound_full/images/56.png\n",
            "../validation_dataset/FS_LostFound_full/images/0.png\n",
            "../validation_dataset/FS_LostFound_full/images/49.png\n",
            "../validation_dataset/FS_LostFound_full/images/4.png\n",
            "../validation_dataset/FS_LostFound_full/images/8.png\n",
            "../validation_dataset/FS_LostFound_full/images/99.png\n",
            "../validation_dataset/FS_LostFound_full/images/61.png\n",
            "../validation_dataset/FS_LostFound_full/images/50.png\n",
            "../validation_dataset/FS_LostFound_full/images/82.png\n",
            "../validation_dataset/FS_LostFound_full/images/80.png\n",
            "../validation_dataset/FS_LostFound_full/images/11.png\n",
            "../validation_dataset/FS_LostFound_full/images/73.png\n",
            "../validation_dataset/FS_LostFound_full/images/60.png\n",
            "../validation_dataset/FS_LostFound_full/images/41.png\n",
            "../validation_dataset/FS_LostFound_full/images/84.png\n",
            "../validation_dataset/FS_LostFound_full/images/53.png\n",
            "../validation_dataset/FS_LostFound_full/images/75.png\n",
            "../validation_dataset/FS_LostFound_full/images/77.png\n",
            "../validation_dataset/FS_LostFound_full/images/59.png\n",
            "../validation_dataset/FS_LostFound_full/images/44.png\n",
            "../validation_dataset/FS_LostFound_full/images/15.png\n",
            "../validation_dataset/FS_LostFound_full/images/69.png\n",
            "../validation_dataset/FS_LostFound_full/images/23.png\n",
            "../validation_dataset/FS_LostFound_full/images/26.png\n",
            "../validation_dataset/FS_LostFound_full/images/48.png\n",
            "../validation_dataset/FS_LostFound_full/images/40.png\n",
            "Metric: maxEntropy\n",
            "AUPRC score: 2.581709165222814\n",
            "FPR@TPR95: 50.368195005296116\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd eval\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadObsticle21/images/*.webp\" --metric=\"maxEntropy\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly21/images/*.png\" --metric=\"maxEntropy\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly/images/*.jpg\" --metric=\"maxEntropy\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/fs_static/images/*.jpg\" --metric=\"maxEntropy\"\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/FS_LostFound_full/images/*.png\" --metric=\"maxEntropy\"\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZdRxGMJpkw5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssQ4eNYsEWsx"
      },
      "source": [
        "**Model: ERFNet metric: MSP with Temperature scaling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28FLDwBVEeLX",
        "outputId": "04a56cce-0919-41f5-f189-8314826bd9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/eval\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadObsticle21/images/8.webp\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadObsticle21/images/11.webp\n",
            "../validation_dataset/RoadObsticle21/images/7.webp\n",
            "../validation_dataset/RoadObsticle21/images/17.webp\n",
            "../validation_dataset/RoadObsticle21/images/13.webp\n",
            "../validation_dataset/RoadObsticle21/images/9.webp\n",
            "../validation_dataset/RoadObsticle21/images/10.webp\n",
            "../validation_dataset/RoadObsticle21/images/19.webp\n",
            "../validation_dataset/RoadObsticle21/images/18.webp\n",
            "../validation_dataset/RoadObsticle21/images/27.webp\n",
            "../validation_dataset/RoadObsticle21/images/4.webp\n",
            "../validation_dataset/RoadObsticle21/images/28.webp\n",
            "../validation_dataset/RoadObsticle21/images/14.webp\n",
            "../validation_dataset/RoadObsticle21/images/22.webp\n",
            "../validation_dataset/RoadObsticle21/images/1.webp\n",
            "../validation_dataset/RoadObsticle21/images/12.webp\n",
            "../validation_dataset/RoadObsticle21/images/25.webp\n",
            "../validation_dataset/RoadObsticle21/images/26.webp\n",
            "../validation_dataset/RoadObsticle21/images/3.webp\n",
            "../validation_dataset/RoadObsticle21/images/29.webp\n",
            "../validation_dataset/RoadObsticle21/images/16.webp\n",
            "../validation_dataset/RoadObsticle21/images/0.webp\n",
            "../validation_dataset/RoadObsticle21/images/6.webp\n",
            "../validation_dataset/RoadObsticle21/images/24.webp\n",
            "../validation_dataset/RoadObsticle21/images/5.webp\n",
            "../validation_dataset/RoadObsticle21/images/23.webp\n",
            "../validation_dataset/RoadObsticle21/images/20.webp\n",
            "../validation_dataset/RoadObsticle21/images/21.webp\n",
            "../validation_dataset/RoadObsticle21/images/2.webp\n",
            "../validation_dataset/RoadObsticle21/images/15.webp\n",
            "Metric: msp\n",
            "AUPRC score: 2.9873807437711215\n",
            "FPR@TPR95: 69.28503706328912\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly21/images/2.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly21/images/3.png\n",
            "../validation_dataset/RoadAnomaly21/images/9.png\n",
            "../validation_dataset/RoadAnomaly21/images/7.png\n",
            "../validation_dataset/RoadAnomaly21/images/1.png\n",
            "../validation_dataset/RoadAnomaly21/images/6.png\n",
            "../validation_dataset/RoadAnomaly21/images/5.png\n",
            "../validation_dataset/RoadAnomaly21/images/0.png\n",
            "../validation_dataset/RoadAnomaly21/images/4.png\n",
            "../validation_dataset/RoadAnomaly21/images/8.png\n",
            "Metric: msp\n",
            "AUPRC score: 30.49061373422423\n",
            "FPR@TPR95: 63.816120039163174\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/RoadAnomaly/images/14.jpg\n",
            "../validation_dataset/RoadAnomaly/images/4.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/RoadAnomaly/images/30.jpg\n",
            "../validation_dataset/RoadAnomaly/images/49.jpg\n",
            "../validation_dataset/RoadAnomaly/images/40.jpg\n",
            "../validation_dataset/RoadAnomaly/images/50.jpg\n",
            "../validation_dataset/RoadAnomaly/images/12.jpg\n",
            "../validation_dataset/RoadAnomaly/images/15.jpg\n",
            "../validation_dataset/RoadAnomaly/images/27.jpg\n",
            "../validation_dataset/RoadAnomaly/images/10.jpg\n",
            "../validation_dataset/RoadAnomaly/images/23.jpg\n",
            "../validation_dataset/RoadAnomaly/images/22.jpg\n",
            "../validation_dataset/RoadAnomaly/images/39.jpg\n",
            "../validation_dataset/RoadAnomaly/images/18.jpg\n",
            "../validation_dataset/RoadAnomaly/images/58.jpg\n",
            "../validation_dataset/RoadAnomaly/images/19.jpg\n",
            "../validation_dataset/RoadAnomaly/images/51.jpg\n",
            "../validation_dataset/RoadAnomaly/images/1.jpg\n",
            "../validation_dataset/RoadAnomaly/images/36.jpg\n",
            "../validation_dataset/RoadAnomaly/images/35.jpg\n",
            "../validation_dataset/RoadAnomaly/images/42.jpg\n",
            "../validation_dataset/RoadAnomaly/images/53.jpg\n",
            "../validation_dataset/RoadAnomaly/images/44.jpg\n",
            "../validation_dataset/RoadAnomaly/images/33.jpg\n",
            "../validation_dataset/RoadAnomaly/images/46.jpg\n",
            "../validation_dataset/RoadAnomaly/images/0.jpg\n",
            "../validation_dataset/RoadAnomaly/images/3.jpg\n",
            "../validation_dataset/RoadAnomaly/images/55.jpg\n",
            "../validation_dataset/RoadAnomaly/images/24.jpg\n",
            "../validation_dataset/RoadAnomaly/images/11.jpg\n",
            "../validation_dataset/RoadAnomaly/images/43.jpg\n",
            "../validation_dataset/RoadAnomaly/images/34.jpg\n",
            "../validation_dataset/RoadAnomaly/images/5.jpg\n",
            "../validation_dataset/RoadAnomaly/images/41.jpg\n",
            "../validation_dataset/RoadAnomaly/images/56.jpg\n",
            "../validation_dataset/RoadAnomaly/images/54.jpg\n",
            "../validation_dataset/RoadAnomaly/images/37.jpg\n",
            "../validation_dataset/RoadAnomaly/images/29.jpg\n",
            "../validation_dataset/RoadAnomaly/images/8.jpg\n",
            "../validation_dataset/RoadAnomaly/images/2.jpg\n",
            "../validation_dataset/RoadAnomaly/images/25.jpg\n",
            "../validation_dataset/RoadAnomaly/images/28.jpg\n",
            "../validation_dataset/RoadAnomaly/images/7.jpg\n",
            "../validation_dataset/RoadAnomaly/images/13.jpg\n",
            "../validation_dataset/RoadAnomaly/images/45.jpg\n",
            "../validation_dataset/RoadAnomaly/images/38.jpg\n",
            "../validation_dataset/RoadAnomaly/images/20.jpg\n",
            "../validation_dataset/RoadAnomaly/images/47.jpg\n",
            "../validation_dataset/RoadAnomaly/images/17.jpg\n",
            "../validation_dataset/RoadAnomaly/images/57.jpg\n",
            "../validation_dataset/RoadAnomaly/images/9.jpg\n",
            "../validation_dataset/RoadAnomaly/images/48.jpg\n",
            "../validation_dataset/RoadAnomaly/images/26.jpg\n",
            "../validation_dataset/RoadAnomaly/images/16.jpg\n",
            "../validation_dataset/RoadAnomaly/images/21.jpg\n",
            "../validation_dataset/RoadAnomaly/images/6.jpg\n",
            "../validation_dataset/RoadAnomaly/images/31.jpg\n",
            "../validation_dataset/RoadAnomaly/images/52.jpg\n",
            "../validation_dataset/RoadAnomaly/images/59.jpg\n",
            "../validation_dataset/RoadAnomaly/images/32.jpg\n",
            "Metric: msp\n",
            "AUPRC score: 12.633704514894436\n",
            "FPR@TPR95: 83.60317107247248\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/fs_static/images/14.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/fs_static/images/4.jpg\n",
            "../validation_dataset/fs_static/images/12.jpg\n",
            "../validation_dataset/fs_static/images/15.jpg\n",
            "../validation_dataset/fs_static/images/27.jpg\n",
            "../validation_dataset/fs_static/images/10.jpg\n",
            "../validation_dataset/fs_static/images/23.jpg\n",
            "../validation_dataset/fs_static/images/22.jpg\n",
            "../validation_dataset/fs_static/images/18.jpg\n",
            "../validation_dataset/fs_static/images/19.jpg\n",
            "../validation_dataset/fs_static/images/1.jpg\n",
            "../validation_dataset/fs_static/images/0.jpg\n",
            "../validation_dataset/fs_static/images/3.jpg\n",
            "../validation_dataset/fs_static/images/24.jpg\n",
            "../validation_dataset/fs_static/images/11.jpg\n",
            "../validation_dataset/fs_static/images/5.jpg\n",
            "../validation_dataset/fs_static/images/29.jpg\n",
            "../validation_dataset/fs_static/images/8.jpg\n",
            "../validation_dataset/fs_static/images/2.jpg\n",
            "../validation_dataset/fs_static/images/25.jpg\n",
            "../validation_dataset/fs_static/images/28.jpg\n",
            "../validation_dataset/fs_static/images/7.jpg\n",
            "../validation_dataset/fs_static/images/13.jpg\n",
            "../validation_dataset/fs_static/images/20.jpg\n",
            "../validation_dataset/fs_static/images/17.jpg\n",
            "../validation_dataset/fs_static/images/9.jpg\n",
            "../validation_dataset/fs_static/images/26.jpg\n",
            "../validation_dataset/fs_static/images/16.jpg\n",
            "../validation_dataset/fs_static/images/21.jpg\n",
            "../validation_dataset/fs_static/images/6.jpg\n",
            "Metric: msp\n",
            "AUPRC score: 8.988805863725414\n",
            "FPR@TPR95: 40.99556759453206\n",
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "../validation_dataset/FS_LostFound_full/images/21.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/eval/evalAnomaly.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../validation_dataset/FS_LostFound_full/images/46.png\n",
            "../validation_dataset/FS_LostFound_full/images/24.png\n",
            "../validation_dataset/FS_LostFound_full/images/28.png\n",
            "../validation_dataset/FS_LostFound_full/images/32.png\n",
            "../validation_dataset/FS_LostFound_full/images/58.png\n",
            "../validation_dataset/FS_LostFound_full/images/2.png\n",
            "../validation_dataset/FS_LostFound_full/images/31.png\n",
            "../validation_dataset/FS_LostFound_full/images/33.png\n",
            "../validation_dataset/FS_LostFound_full/images/12.png\n",
            "../validation_dataset/FS_LostFound_full/images/94.png\n",
            "../validation_dataset/FS_LostFound_full/images/86.png\n",
            "../validation_dataset/FS_LostFound_full/images/90.png\n",
            "../validation_dataset/FS_LostFound_full/images/47.png\n",
            "../validation_dataset/FS_LostFound_full/images/93.png\n",
            "../validation_dataset/FS_LostFound_full/images/27.png\n",
            "../validation_dataset/FS_LostFound_full/images/68.png\n",
            "../validation_dataset/FS_LostFound_full/images/97.png\n",
            "../validation_dataset/FS_LostFound_full/images/62.png\n",
            "../validation_dataset/FS_LostFound_full/images/85.png\n",
            "../validation_dataset/FS_LostFound_full/images/38.png\n",
            "../validation_dataset/FS_LostFound_full/images/91.png\n",
            "../validation_dataset/FS_LostFound_full/images/74.png\n",
            "../validation_dataset/FS_LostFound_full/images/66.png\n",
            "../validation_dataset/FS_LostFound_full/images/37.png\n",
            "../validation_dataset/FS_LostFound_full/images/30.png\n",
            "../validation_dataset/FS_LostFound_full/images/3.png\n",
            "../validation_dataset/FS_LostFound_full/images/29.png\n",
            "../validation_dataset/FS_LostFound_full/images/87.png\n",
            "../validation_dataset/FS_LostFound_full/images/13.png\n",
            "../validation_dataset/FS_LostFound_full/images/14.png\n",
            "../validation_dataset/FS_LostFound_full/images/92.png\n",
            "../validation_dataset/FS_LostFound_full/images/64.png\n",
            "../validation_dataset/FS_LostFound_full/images/67.png\n",
            "../validation_dataset/FS_LostFound_full/images/22.png\n",
            "../validation_dataset/FS_LostFound_full/images/72.png\n",
            "../validation_dataset/FS_LostFound_full/images/65.png\n",
            "../validation_dataset/FS_LostFound_full/images/81.png\n",
            "../validation_dataset/FS_LostFound_full/images/34.png\n",
            "../validation_dataset/FS_LostFound_full/images/20.png\n",
            "../validation_dataset/FS_LostFound_full/images/16.png\n",
            "../validation_dataset/FS_LostFound_full/images/43.png\n",
            "../validation_dataset/FS_LostFound_full/images/17.png\n",
            "../validation_dataset/FS_LostFound_full/images/88.png\n",
            "../validation_dataset/FS_LostFound_full/images/9.png\n",
            "../validation_dataset/FS_LostFound_full/images/42.png\n",
            "../validation_dataset/FS_LostFound_full/images/7.png\n",
            "../validation_dataset/FS_LostFound_full/images/1.png\n",
            "../validation_dataset/FS_LostFound_full/images/6.png\n",
            "../validation_dataset/FS_LostFound_full/images/55.png\n",
            "../validation_dataset/FS_LostFound_full/images/51.png\n",
            "../validation_dataset/FS_LostFound_full/images/10.png\n",
            "../validation_dataset/FS_LostFound_full/images/36.png\n",
            "../validation_dataset/FS_LostFound_full/images/95.png\n",
            "../validation_dataset/FS_LostFound_full/images/98.png\n",
            "../validation_dataset/FS_LostFound_full/images/35.png\n",
            "../validation_dataset/FS_LostFound_full/images/76.png\n",
            "../validation_dataset/FS_LostFound_full/images/78.png\n",
            "../validation_dataset/FS_LostFound_full/images/52.png\n",
            "../validation_dataset/FS_LostFound_full/images/45.png\n",
            "../validation_dataset/FS_LostFound_full/images/54.png\n",
            "../validation_dataset/FS_LostFound_full/images/83.png\n",
            "../validation_dataset/FS_LostFound_full/images/5.png\n",
            "../validation_dataset/FS_LostFound_full/images/79.png\n",
            "../validation_dataset/FS_LostFound_full/images/96.png\n",
            "../validation_dataset/FS_LostFound_full/images/71.png\n",
            "../validation_dataset/FS_LostFound_full/images/89.png\n",
            "../validation_dataset/FS_LostFound_full/images/18.png\n",
            "../validation_dataset/FS_LostFound_full/images/57.png\n",
            "../validation_dataset/FS_LostFound_full/images/25.png\n",
            "../validation_dataset/FS_LostFound_full/images/19.png\n",
            "../validation_dataset/FS_LostFound_full/images/63.png\n",
            "../validation_dataset/FS_LostFound_full/images/70.png\n",
            "../validation_dataset/FS_LostFound_full/images/39.png\n",
            "../validation_dataset/FS_LostFound_full/images/56.png\n",
            "../validation_dataset/FS_LostFound_full/images/0.png\n",
            "../validation_dataset/FS_LostFound_full/images/49.png\n",
            "../validation_dataset/FS_LostFound_full/images/4.png\n",
            "../validation_dataset/FS_LostFound_full/images/8.png\n",
            "../validation_dataset/FS_LostFound_full/images/99.png\n",
            "../validation_dataset/FS_LostFound_full/images/61.png\n",
            "../validation_dataset/FS_LostFound_full/images/50.png\n",
            "../validation_dataset/FS_LostFound_full/images/82.png\n",
            "../validation_dataset/FS_LostFound_full/images/80.png\n",
            "../validation_dataset/FS_LostFound_full/images/11.png\n",
            "../validation_dataset/FS_LostFound_full/images/73.png\n",
            "../validation_dataset/FS_LostFound_full/images/60.png\n",
            "../validation_dataset/FS_LostFound_full/images/41.png\n",
            "../validation_dataset/FS_LostFound_full/images/84.png\n",
            "../validation_dataset/FS_LostFound_full/images/53.png\n",
            "../validation_dataset/FS_LostFound_full/images/75.png\n",
            "../validation_dataset/FS_LostFound_full/images/77.png\n",
            "../validation_dataset/FS_LostFound_full/images/59.png\n",
            "../validation_dataset/FS_LostFound_full/images/44.png\n",
            "../validation_dataset/FS_LostFound_full/images/15.png\n",
            "../validation_dataset/FS_LostFound_full/images/69.png\n",
            "../validation_dataset/FS_LostFound_full/images/23.png\n",
            "../validation_dataset/FS_LostFound_full/images/26.png\n",
            "../validation_dataset/FS_LostFound_full/images/48.png\n",
            "../validation_dataset/FS_LostFound_full/images/40.png\n",
            "Metric: msp\n",
            "AUPRC score: 2.46444102051976\n",
            "FPR@TPR95: 48.98480769611621\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd eval\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadObsticle21/images/*.webp\" --metric=\"msp\" --temperature=1.7\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly21/images/*.png\" --metric=\"msp\" --temperature=1.7\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/RoadAnomaly/images/*.jpg\" --metric=\"msp\" --temperature=1.7\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/fs_static/images/*.jpg\" --metric=\"msp\" --temperature=1.7\n",
        "%run evalAnomaly.py --input=\"../validation_dataset/FS_LostFound_full/images/*.png\" --metric=\"msp\" --temperature=1.7\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbgwk-zu7ZYf"
      },
      "source": [
        "**Cityscapes Dataset mIoU model: ERFNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF0-JSJ94SbL"
      },
      "outputs": [],
      "source": [
        "%cd eval\n",
        "%run eval_iou.py --datadir \"../cityscapes/\" --subset \"val\"\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxJ2A-vhZi3o"
      },
      "source": [
        "**Finetuning ERFNET Void Clasiifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmM78MdB8NFM",
        "outputId": "e2d89a5e-3007-445e-98f7-a5989d510060"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/train/main.py:542: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model, torch.load(weights_path))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== DECODER TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "tensor([ 2.8149,  6.9850,  3.7890,  9.9428,  9.7702,  9.5111, 10.3114, 10.0265,\n",
            "         4.6323,  9.5608,  7.8698,  9.5169, 10.3737,  6.6616, 10.2605, 10.2879,\n",
            "        10.2898, 10.4054, 10.1381,  1.0000], device='cuda:0')\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/train/main.py:99: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n",
            "  self.loss = torch.nn.NLLLoss2d(weight)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.3586 (epoch: 1, step: 0) // Avg time/img: 0.3342 s\n",
            "loss: 0.3331 (epoch: 1, step: 50) // Avg time/img: 0.1047 s\n",
            "loss: 0.309 (epoch: 1, step: 100) // Avg time/img: 0.1050 s\n",
            "loss: 0.3 (epoch: 1, step: 150) // Avg time/img: 0.1063 s\n",
            "loss: 0.2916 (epoch: 1, step: 200) // Avg time/img: 0.1068 s\n",
            "----- VALIDATING - EPOCH 1 -----\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/train/main.py:356: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
            "/content/train/main.py:357: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  targets = Variable(labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL loss: 0.3873 (epoch: 1, step: 0) // Avg time/img: 0.0420 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.00\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_finetuned1_30/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.849746889841331e-05\n",
            "loss: 0.2951 (epoch: 2, step: 0) // Avg time/img: 0.1147 s\n",
            "loss: 0.2581 (epoch: 2, step: 50) // Avg time/img: 0.1087 s\n",
            "loss: 0.253 (epoch: 2, step: 100) // Avg time/img: 0.1085 s\n",
            "loss: 0.2512 (epoch: 2, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.2503 (epoch: 2, step: 200) // Avg time/img: 0.1085 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.3561 (epoch: 2, step: 0) // Avg time/img: 0.0401 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.68\u001b[0m %\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.698974662943611e-05\n",
            "loss: 0.2245 (epoch: 3, step: 0) // Avg time/img: 0.1122 s\n",
            "loss: 0.2342 (epoch: 3, step: 50) // Avg time/img: 0.1084 s\n",
            "loss: 0.2343 (epoch: 3, step: 100) // Avg time/img: 0.1085 s\n",
            "loss: 0.2375 (epoch: 3, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.2374 (epoch: 3, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.3665 (epoch: 3, step: 0) // Avg time/img: 0.0433 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.85\u001b[0m %\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 0.272 (epoch: 4, step: 0) // Avg time/img: 0.1138 s\n",
            "loss: 0.233 (epoch: 4, step: 50) // Avg time/img: 0.1088 s\n",
            "loss: 0.2288 (epoch: 4, step: 100) // Avg time/img: 0.1087 s\n",
            "loss: 0.2289 (epoch: 4, step: 150) // Avg time/img: 0.1087 s\n",
            "loss: 0.2293 (epoch: 4, step: 200) // Avg time/img: 0.1086 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.3693 (epoch: 4, step: 0) // Avg time/img: 0.0409 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m69.65\u001b[0m %\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.3957895096839957e-05\n",
            "loss: 0.2453 (epoch: 5, step: 0) // Avg time/img: 0.1139 s\n",
            "loss: 0.2282 (epoch: 5, step: 50) // Avg time/img: 0.1083 s\n",
            "loss: 0.226 (epoch: 5, step: 100) // Avg time/img: 0.1084 s\n",
            "loss: 0.2233 (epoch: 5, step: 150) // Avg time/img: 0.1087 s\n",
            "loss: 0.2209 (epoch: 5, step: 200) // Avg time/img: 0.1086 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.3587 (epoch: 5, step: 0) // Avg time/img: 0.0526 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.90\u001b[0m %\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  4.243330733945934e-05\n",
            "loss: 0.1823 (epoch: 6, step: 0) // Avg time/img: 0.1143 s\n",
            "loss: 0.2097 (epoch: 6, step: 50) // Avg time/img: 0.1088 s\n",
            "loss: 0.2065 (epoch: 6, step: 100) // Avg time/img: 0.1089 s\n",
            "loss: 0.2077 (epoch: 6, step: 150) // Avg time/img: 0.1088 s\n",
            "loss: 0.2058 (epoch: 6, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.3163 (epoch: 6, step: 0) // Avg time/img: 0.0428 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.08\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_finetuned1_30/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 0.2295 (epoch: 7, step: 0) // Avg time/img: 0.1105 s\n",
            "loss: 0.183 (epoch: 7, step: 50) // Avg time/img: 0.1089 s\n",
            "loss: 0.1752 (epoch: 7, step: 100) // Avg time/img: 0.1088 s\n",
            "loss: 0.1742 (epoch: 7, step: 150) // Avg time/img: 0.1088 s\n",
            "loss: 0.1713 (epoch: 7, step: 200) // Avg time/img: 0.1089 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.286 (epoch: 7, step: 0) // Avg time/img: 0.0413 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.82\u001b[0m %\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.936551409577103e-05\n",
            "loss: 0.1676 (epoch: 8, step: 0) // Avg time/img: 0.1147 s\n",
            "loss: 0.1612 (epoch: 8, step: 50) // Avg time/img: 0.1087 s\n",
            "loss: 0.162 (epoch: 8, step: 100) // Avg time/img: 0.1086 s\n",
            "loss: 0.1602 (epoch: 8, step: 150) // Avg time/img: 0.1087 s\n",
            "loss: 0.1595 (epoch: 8, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.2684 (epoch: 8, step: 0) // Avg time/img: 0.0441 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.37\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_finetuned1_30/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.7821721103476616e-05\n",
            "loss: 0.1694 (epoch: 9, step: 0) // Avg time/img: 0.1093 s\n",
            "loss: 0.1571 (epoch: 9, step: 50) // Avg time/img: 0.1083 s\n",
            "loss: 0.1588 (epoch: 9, step: 100) // Avg time/img: 0.1085 s\n",
            "loss: 0.1568 (epoch: 9, step: 150) // Avg time/img: 0.1084 s\n",
            "loss: 0.1555 (epoch: 9, step: 200) // Avg time/img: 0.1084 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.2625 (epoch: 9, step: 0) // Avg time/img: 0.0404 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.32\u001b[0m %\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.1178 (epoch: 10, step: 0) // Avg time/img: 0.1125 s\n",
            "loss: 0.1512 (epoch: 10, step: 50) // Avg time/img: 0.1084 s\n",
            "loss: 0.1516 (epoch: 10, step: 100) // Avg time/img: 0.1084 s\n",
            "loss: 0.1524 (epoch: 10, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.1518 (epoch: 10, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.2395 (epoch: 10, step: 0) // Avg time/img: 0.0380 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.36\u001b[0m %\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  3.471265813308036e-05\n",
            "loss: 0.1416 (epoch: 11, step: 0) // Avg time/img: 0.1114 s\n",
            "loss: 0.1487 (epoch: 11, step: 50) // Avg time/img: 0.1087 s\n",
            "loss: 0.1493 (epoch: 11, step: 100) // Avg time/img: 0.1086 s\n",
            "loss: 0.1496 (epoch: 11, step: 150) // Avg time/img: 0.1087 s\n",
            "loss: 0.1504 (epoch: 11, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.2677 (epoch: 11, step: 0) // Avg time/img: 0.0429 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.39\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_finetuned1_30/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  3.3146609808503096e-05\n",
            "loss: 0.1739 (epoch: 12, step: 0) // Avg time/img: 0.1169 s\n",
            "loss: 0.1494 (epoch: 12, step: 50) // Avg time/img: 0.1086 s\n",
            "loss: 0.1504 (epoch: 12, step: 100) // Avg time/img: 0.1087 s\n",
            "loss: 0.1485 (epoch: 12, step: 150) // Avg time/img: 0.1088 s\n",
            "loss: 0.1486 (epoch: 12, step: 200) // Avg time/img: 0.1089 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.2495 (epoch: 12, step: 0) // Avg time/img: 0.0398 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.24\u001b[0m %\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.173 (epoch: 13, step: 0) // Avg time/img: 0.1146 s\n",
            "loss: 0.1513 (epoch: 13, step: 50) // Avg time/img: 0.1086 s\n",
            "loss: 0.1486 (epoch: 13, step: 100) // Avg time/img: 0.1084 s\n",
            "loss: 0.1485 (epoch: 13, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.1485 (epoch: 13, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.2712 (epoch: 13, step: 0) // Avg time/img: 0.0404 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.43\u001b[0m %\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  2.998920164149494e-05\n",
            "loss: 0.1318 (epoch: 14, step: 0) // Avg time/img: 0.1214 s\n",
            "loss: 0.1459 (epoch: 14, step: 50) // Avg time/img: 0.1089 s\n",
            "loss: 0.1468 (epoch: 14, step: 100) // Avg time/img: 0.1086 s\n",
            "loss: 0.1475 (epoch: 14, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.1476 (epoch: 14, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.2502 (epoch: 14, step: 0) // Avg time/img: 0.0478 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.32\u001b[0m %\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  2.8396764480896166e-05\n",
            "loss: 0.1551 (epoch: 15, step: 0) // Avg time/img: 0.1109 s\n",
            "loss: 0.1498 (epoch: 15, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1469 (epoch: 15, step: 100) // Avg time/img: 0.1086 s\n",
            "loss: 0.1462 (epoch: 15, step: 150) // Avg time/img: 0.1084 s\n",
            "loss: 0.1461 (epoch: 15, step: 200) // Avg time/img: 0.1084 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.2591 (epoch: 15, step: 0) // Avg time/img: 0.0419 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.45\u001b[0m %\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.2006 (epoch: 16, step: 0) // Avg time/img: 0.1159 s\n",
            "loss: 0.1462 (epoch: 16, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1448 (epoch: 16, step: 100) // Avg time/img: 0.1084 s\n",
            "loss: 0.1458 (epoch: 16, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.1458 (epoch: 16, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.2678 (epoch: 16, step: 0) // Avg time/img: 0.0429 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.29\u001b[0m %\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  2.5181181724366927e-05\n",
            "loss: 0.1465 (epoch: 17, step: 0) // Avg time/img: 0.1142 s\n",
            "loss: 0.1419 (epoch: 17, step: 50) // Avg time/img: 0.1086 s\n",
            "loss: 0.1435 (epoch: 17, step: 100) // Avg time/img: 0.1088 s\n",
            "loss: 0.1446 (epoch: 17, step: 150) // Avg time/img: 0.1088 s\n",
            "loss: 0.144 (epoch: 17, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.2469 (epoch: 17, step: 0) // Avg time/img: 0.0427 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.49\u001b[0m %\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  2.3556452716873652e-05\n",
            "loss: 0.1638 (epoch: 18, step: 0) // Avg time/img: 0.1196 s\n",
            "loss: 0.143 (epoch: 18, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1433 (epoch: 18, step: 100) // Avg time/img: 0.1086 s\n",
            "loss: 0.1439 (epoch: 18, step: 150) // Avg time/img: 0.1087 s\n",
            "loss: 0.1436 (epoch: 18, step: 200) // Avg time/img: 0.1086 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.2725 (epoch: 18, step: 0) // Avg time/img: 0.0416 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.19\u001b[0m %\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.1975 (epoch: 19, step: 0) // Avg time/img: 0.1106 s\n",
            "loss: 0.1424 (epoch: 19, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1432 (epoch: 19, step: 100) // Avg time/img: 0.1085 s\n",
            "loss: 0.1438 (epoch: 19, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.1436 (epoch: 19, step: 200) // Avg time/img: 0.1086 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.2654 (epoch: 19, step: 0) // Avg time/img: 0.0432 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.20\u001b[0m %\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  2.026815849307756e-05\n",
            "loss: 0.1433 (epoch: 20, step: 0) // Avg time/img: 0.1138 s\n",
            "loss: 0.1407 (epoch: 20, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1426 (epoch: 20, step: 100) // Avg time/img: 0.1086 s\n",
            "loss: 0.1428 (epoch: 20, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.1431 (epoch: 20, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.2511 (epoch: 20, step: 0) // Avg time/img: 0.0432 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.89\u001b[0m %\n",
            "----- TRAINING - EPOCH 21 -----\n",
            "LEARNING RATE:  1.8602052900565075e-05\n",
            "loss: 0.1555 (epoch: 21, step: 0) // Avg time/img: 0.1085 s\n",
            "loss: 0.1397 (epoch: 21, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1407 (epoch: 21, step: 100) // Avg time/img: 0.1088 s\n",
            "loss: 0.1418 (epoch: 21, step: 150) // Avg time/img: 0.1089 s\n",
            "loss: 0.1416 (epoch: 21, step: 200) // Avg time/img: 0.1088 s\n",
            "----- VALIDATING - EPOCH 21 -----\n",
            "VAL loss: 0.2781 (epoch: 21, step: 0) // Avg time/img: 0.0433 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.94\u001b[0m %\n",
            "----- TRAINING - EPOCH 22 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.1479 (epoch: 22, step: 0) // Avg time/img: 0.1144 s\n",
            "loss: 0.1438 (epoch: 22, step: 50) // Avg time/img: 0.1088 s\n",
            "loss: 0.1419 (epoch: 22, step: 100) // Avg time/img: 0.1090 s\n",
            "loss: 0.1415 (epoch: 22, step: 150) // Avg time/img: 0.1089 s\n",
            "loss: 0.1414 (epoch: 22, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 22 -----\n",
            "VAL loss: 0.2579 (epoch: 22, step: 0) // Avg time/img: 0.0425 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.52\u001b[0m %\n",
            "----- TRAINING - EPOCH 23 -----\n",
            "LEARNING RATE:  1.5217449296258857e-05\n",
            "loss: 0.1375 (epoch: 23, step: 0) // Avg time/img: 0.1131 s\n",
            "loss: 0.1419 (epoch: 23, step: 50) // Avg time/img: 0.1088 s\n",
            "loss: 0.1431 (epoch: 23, step: 100) // Avg time/img: 0.1087 s\n",
            "loss: 0.1401 (epoch: 23, step: 150) // Avg time/img: 0.1087 s\n",
            "loss: 0.1409 (epoch: 23, step: 200) // Avg time/img: 0.1088 s\n",
            "----- VALIDATING - EPOCH 23 -----\n",
            "VAL loss: 0.2614 (epoch: 23, step: 0) // Avg time/img: 0.0368 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.44\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_finetuned1_30/model_best.pth (epoch: 23)\n",
            "----- TRAINING - EPOCH 24 -----\n",
            "LEARNING RATE:  1.3494261163740182e-05\n",
            "loss: 0.1408 (epoch: 24, step: 0) // Avg time/img: 0.1150 s\n",
            "loss: 0.1376 (epoch: 24, step: 50) // Avg time/img: 0.1095 s\n",
            "loss: 0.1377 (epoch: 24, step: 100) // Avg time/img: 0.1089 s\n",
            "loss: 0.1385 (epoch: 24, step: 150) // Avg time/img: 0.1087 s\n",
            "loss: 0.1396 (epoch: 24, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 24 -----\n",
            "VAL loss: 0.2494 (epoch: 24, step: 0) // Avg time/img: 0.0380 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m70.92\u001b[0m %\n",
            "----- TRAINING - EPOCH 25 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.1344 (epoch: 25, step: 0) // Avg time/img: 0.1182 s\n",
            "loss: 0.1384 (epoch: 25, step: 50) // Avg time/img: 0.1086 s\n",
            "loss: 0.1381 (epoch: 25, step: 100) // Avg time/img: 0.1085 s\n",
            "loss: 0.1387 (epoch: 25, step: 150) // Avg time/img: 0.1085 s\n",
            "loss: 0.1396 (epoch: 25, step: 200) // Avg time/img: 0.1086 s\n",
            "----- VALIDATING - EPOCH 25 -----\n",
            "VAL loss: 0.2542 (epoch: 25, step: 0) // Avg time/img: 0.0516 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.47\u001b[0m %\n",
            "----- TRAINING - EPOCH 26 -----\n",
            "LEARNING RATE:  9.96859332376096e-06\n",
            "loss: 0.1411 (epoch: 26, step: 0) // Avg time/img: 0.1111 s\n",
            "loss: 0.1386 (epoch: 26, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1387 (epoch: 26, step: 100) // Avg time/img: 0.1087 s\n",
            "loss: 0.1392 (epoch: 26, step: 150) // Avg time/img: 0.1086 s\n",
            "loss: 0.1395 (epoch: 26, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 26 -----\n",
            "VAL loss: 0.2619 (epoch: 26, step: 0) // Avg time/img: 0.0422 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.82\u001b[0m %\n",
            "----- TRAINING - EPOCH 27 -----\n",
            "LEARNING RATE:  8.154829161610913e-06\n",
            "loss: 0.1398 (epoch: 27, step: 0) // Avg time/img: 0.1087 s\n",
            "loss: 0.1409 (epoch: 27, step: 50) // Avg time/img: 0.1083 s\n",
            "loss: 0.1399 (epoch: 27, step: 100) // Avg time/img: 0.1084 s\n",
            "loss: 0.1405 (epoch: 27, step: 150) // Avg time/img: 0.1084 s\n",
            "loss: 0.1398 (epoch: 27, step: 200) // Avg time/img: 0.1087 s\n",
            "----- VALIDATING - EPOCH 27 -----\n",
            "VAL loss: 0.2609 (epoch: 27, step: 0) // Avg time/img: 0.0408 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.23\u001b[0m %\n",
            "----- TRAINING - EPOCH 28 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.1347 (epoch: 28, step: 0) // Avg time/img: 0.1205 s\n",
            "loss: 0.1368 (epoch: 28, step: 50) // Avg time/img: 0.1085 s\n",
            "loss: 0.1367 (epoch: 28, step: 100) // Avg time/img: 0.1082 s\n",
            "loss: 0.1369 (epoch: 28, step: 150) // Avg time/img: 0.1085 s\n",
            "loss: 0.1385 (epoch: 28, step: 200) // Avg time/img: 0.1083 s\n",
            "----- VALIDATING - EPOCH 28 -----\n",
            "VAL loss: 0.266 (epoch: 28, step: 0) // Avg time/img: 0.0458 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.20\u001b[0m %\n",
            "----- TRAINING - EPOCH 29 -----\n",
            "LEARNING RATE:  4.370064743465832e-06\n",
            "loss: 0.1501 (epoch: 29, step: 0) // Avg time/img: 0.1159 s\n",
            "loss: 0.1387 (epoch: 29, step: 50) // Avg time/img: 0.1082 s\n",
            "loss: 0.1382 (epoch: 29, step: 100) // Avg time/img: 0.1082 s\n",
            "loss: 0.138 (epoch: 29, step: 150) // Avg time/img: 0.1085 s\n",
            "loss: 0.1382 (epoch: 29, step: 200) // Avg time/img: 0.1086 s\n",
            "----- VALIDATING - EPOCH 29 -----\n",
            "VAL loss: 0.2588 (epoch: 29, step: 0) // Avg time/img: 0.0458 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.19\u001b[0m %\n",
            "----- TRAINING - EPOCH 30 -----\n",
            "LEARNING RATE:  2.341859710806076e-06\n",
            "loss: 0.1516 (epoch: 30, step: 0) // Avg time/img: 0.1119 s\n",
            "loss: 0.1349 (epoch: 30, step: 50) // Avg time/img: 0.1084 s\n",
            "loss: 0.1375 (epoch: 30, step: 100) // Avg time/img: 0.1083 s\n",
            "loss: 0.1379 (epoch: 30, step: 150) // Avg time/img: 0.1083 s\n",
            "loss: 0.1379 (epoch: 30, step: 200) // Avg time/img: 0.1085 s\n",
            "----- VALIDATING - EPOCH 30 -----\n",
            "VAL loss: 0.249 (epoch: 30, step: 0) // Avg time/img: 0.0424 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m72.12\u001b[0m %\n",
            "========== TRAINING FINISHED ===========\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\" --num-epochs 30 --savedir \"erfnet_finetuned1_30\" --batch-size=12 --pretrained --decoder\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMNjz7blaATT"
      },
      "source": [
        "**ENET Finetuning Void Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EIUELqsZsj0",
        "outputId": "ea706057-d59c-4221-e1e8-af061589268e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/train/main.py:544: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model.module, torch.load(weights_path))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cur_epoch  not loaded\n",
            "best_score  not loaded\n",
            "state_dict  not loaded\n",
            "optimizer  not loaded\n",
            "scheduler  not loaded\n",
            "========== DECODER TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "tensor([ 3.3637, 14.0423,  4.9949, 39.2600, 36.5153, 32.9067, 46.2774, 40.6746,\n",
            "         6.7115, 33.5628, 18.5449, 32.9998, 47.6837, 12.7029, 45.2079, 45.7834,\n",
            "        45.8276, 48.4084, 42.7632,  7.8450], device='cuda:0')\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/train/main.py:99: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n",
            "  self.loss = torch.nn.NLLLoss2d(weight)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 3.075 (epoch: 1, step: 0) // Avg time/img: 0.3528 s\n",
            "loss: 3.052 (epoch: 1, step: 50) // Avg time/img: 0.0576 s\n",
            "loss: 2.992 (epoch: 1, step: 100) // Avg time/img: 0.0555 s\n",
            "loss: 2.926 (epoch: 1, step: 150) // Avg time/img: 0.0548 s\n",
            "loss: 2.869 (epoch: 1, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 2.819 (epoch: 1, step: 250) // Avg time/img: 0.0544 s\n",
            "----- VALIDATING - EPOCH 1 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/train/main.py:356: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
            "/content/train/main.py:357: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  targets = Variable(labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL loss: 2.454 (epoch: 1, step: 0) // Avg time/img: 0.0234 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m11.04\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 2.447 (epoch: 2, step: 0) // Avg time/img: 0.0666 s\n",
            "loss: 2.517 (epoch: 2, step: 50) // Avg time/img: 0.0546 s\n",
            "loss: 2.477 (epoch: 2, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 2.455 (epoch: 2, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 2.428 (epoch: 2, step: 200) // Avg time/img: 0.0541 s\n",
            "loss: 2.401 (epoch: 2, step: 250) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 2.124 (epoch: 2, step: 0) // Avg time/img: 0.0225 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 2.242 (epoch: 3, step: 0) // Avg time/img: 0.0812 s\n",
            "loss: 2.235 (epoch: 3, step: 50) // Avg time/img: 0.0549 s\n",
            "loss: 2.208 (epoch: 3, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 2.193 (epoch: 3, step: 150) // Avg time/img: 0.0546 s\n",
            "loss: 2.18 (epoch: 3, step: 200) // Avg time/img: 0.0545 s\n",
            "loss: 2.16 (epoch: 3, step: 250) // Avg time/img: 0.0544 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 1.941 (epoch: 3, step: 0) // Avg time/img: 0.0245 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m21.13\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 1.996 (epoch: 4, step: 0) // Avg time/img: 0.0643 s\n",
            "loss: 2.027 (epoch: 4, step: 50) // Avg time/img: 0.0542 s\n",
            "loss: 2.029 (epoch: 4, step: 100) // Avg time/img: 0.0543 s\n",
            "loss: 2.016 (epoch: 4, step: 150) // Avg time/img: 0.0544 s\n",
            "loss: 2.005 (epoch: 4, step: 200) // Avg time/img: 0.0543 s\n",
            "loss: 1.989 (epoch: 4, step: 250) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 1.812 (epoch: 4, step: 0) // Avg time/img: 0.0231 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m22.72\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 1.828 (epoch: 5, step: 0) // Avg time/img: 0.0656 s\n",
            "loss: 1.889 (epoch: 5, step: 50) // Avg time/img: 0.0548 s\n",
            "loss: 1.89 (epoch: 5, step: 100) // Avg time/img: 0.0542 s\n",
            "loss: 1.884 (epoch: 5, step: 150) // Avg time/img: 0.0542 s\n",
            "loss: 1.882 (epoch: 5, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 1.875 (epoch: 5, step: 250) // Avg time/img: 0.0543 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 1.701 (epoch: 5, step: 0) // Avg time/img: 0.0228 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.01\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.944 (epoch: 6, step: 0) // Avg time/img: 0.0643 s\n",
            "loss: 1.813 (epoch: 6, step: 50) // Avg time/img: 0.0542 s\n",
            "loss: 1.809 (epoch: 6, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.813 (epoch: 6, step: 150) // Avg time/img: 0.0539 s\n",
            "loss: 1.811 (epoch: 6, step: 200) // Avg time/img: 0.0539 s\n",
            "loss: 1.812 (epoch: 6, step: 250) // Avg time/img: 0.0539 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 1.689 (epoch: 6, step: 0) // Avg time/img: 0.0226 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.12\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.89 (epoch: 7, step: 0) // Avg time/img: 0.0721 s\n",
            "loss: 1.812 (epoch: 7, step: 50) // Avg time/img: 0.0544 s\n",
            "loss: 1.804 (epoch: 7, step: 100) // Avg time/img: 0.0543 s\n",
            "loss: 1.809 (epoch: 7, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 1.806 (epoch: 7, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 1.806 (epoch: 7, step: 250) // Avg time/img: 0.0539 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 1.682 (epoch: 7, step: 0) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.09\u001b[0m %\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.598 (epoch: 8, step: 0) // Avg time/img: 0.0558 s\n",
            "loss: 1.806 (epoch: 8, step: 50) // Avg time/img: 0.0541 s\n",
            "loss: 1.793 (epoch: 8, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.798 (epoch: 8, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 1.796 (epoch: 8, step: 200) // Avg time/img: 0.0541 s\n",
            "loss: 1.796 (epoch: 8, step: 250) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 1.68 (epoch: 8, step: 0) // Avg time/img: 0.0212 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.21\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.832 (epoch: 9, step: 0) // Avg time/img: 0.0622 s\n",
            "loss: 1.789 (epoch: 9, step: 50) // Avg time/img: 0.0545 s\n",
            "loss: 1.806 (epoch: 9, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.795 (epoch: 9, step: 150) // Avg time/img: 0.0542 s\n",
            "loss: 1.789 (epoch: 9, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.786 (epoch: 9, step: 250) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 1.666 (epoch: 9, step: 0) // Avg time/img: 0.0230 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.28\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.721 (epoch: 10, step: 0) // Avg time/img: 0.0579 s\n",
            "loss: 1.775 (epoch: 10, step: 50) // Avg time/img: 0.0544 s\n",
            "loss: 1.773 (epoch: 10, step: 100) // Avg time/img: 0.0539 s\n",
            "loss: 1.778 (epoch: 10, step: 150) // Avg time/img: 0.0538 s\n",
            "loss: 1.774 (epoch: 10, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.778 (epoch: 10, step: 250) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.662 (epoch: 10, step: 0) // Avg time/img: 0.0264 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.28\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.881 (epoch: 11, step: 0) // Avg time/img: 0.0625 s\n",
            "loss: 1.789 (epoch: 11, step: 50) // Avg time/img: 0.0548 s\n",
            "loss: 1.782 (epoch: 11, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.777 (epoch: 11, step: 150) // Avg time/img: 0.0544 s\n",
            "loss: 1.772 (epoch: 11, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.769 (epoch: 11, step: 250) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 1.649 (epoch: 11, step: 0) // Avg time/img: 0.0225 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.43\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.821 (epoch: 12, step: 0) // Avg time/img: 0.0805 s\n",
            "loss: 1.79 (epoch: 12, step: 50) // Avg time/img: 0.0549 s\n",
            "loss: 1.776 (epoch: 12, step: 100) // Avg time/img: 0.0542 s\n",
            "loss: 1.77 (epoch: 12, step: 150) // Avg time/img: 0.0542 s\n",
            "loss: 1.771 (epoch: 12, step: 200) // Avg time/img: 0.0541 s\n",
            "loss: 1.774 (epoch: 12, step: 250) // Avg time/img: 0.0542 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 1.649 (epoch: 12, step: 0) // Avg time/img: 0.0192 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.52\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.758 (epoch: 13, step: 0) // Avg time/img: 0.0622 s\n",
            "loss: 1.784 (epoch: 13, step: 50) // Avg time/img: 0.0545 s\n",
            "loss: 1.774 (epoch: 13, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.772 (epoch: 13, step: 150) // Avg time/img: 0.0542 s\n",
            "loss: 1.767 (epoch: 13, step: 200) // Avg time/img: 0.0542 s\n",
            "loss: 1.765 (epoch: 13, step: 250) // Avg time/img: 0.0543 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 1.651 (epoch: 13, step: 0) // Avg time/img: 0.0231 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.51\u001b[0m %\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.861 (epoch: 14, step: 0) // Avg time/img: 0.0666 s\n",
            "loss: 1.79 (epoch: 14, step: 50) // Avg time/img: 0.0548 s\n",
            "loss: 1.78 (epoch: 14, step: 100) // Avg time/img: 0.0542 s\n",
            "loss: 1.771 (epoch: 14, step: 150) // Avg time/img: 0.0542 s\n",
            "loss: 1.77 (epoch: 14, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 1.769 (epoch: 14, step: 250) // Avg time/img: 0.0540 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 1.645 (epoch: 14, step: 0) // Avg time/img: 0.0234 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.73\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_finetuned2/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.743 (epoch: 15, step: 0) // Avg time/img: 0.0592 s\n",
            "loss: 1.772 (epoch: 15, step: 50) // Avg time/img: 0.0535 s\n",
            "loss: 1.766 (epoch: 15, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.766 (epoch: 15, step: 150) // Avg time/img: 0.0544 s\n",
            "loss: 1.767 (epoch: 15, step: 200) // Avg time/img: 0.0541 s\n",
            "loss: 1.767 (epoch: 15, step: 250) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 1.647 (epoch: 15, step: 0) // Avg time/img: 0.0206 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.50\u001b[0m %\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.69 (epoch: 16, step: 0) // Avg time/img: 0.0647 s\n",
            "loss: 1.768 (epoch: 16, step: 50) // Avg time/img: 0.0541 s\n",
            "loss: 1.772 (epoch: 16, step: 100) // Avg time/img: 0.0538 s\n",
            "loss: 1.77 (epoch: 16, step: 150) // Avg time/img: 0.0539 s\n",
            "loss: 1.771 (epoch: 16, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 1.766 (epoch: 16, step: 250) // Avg time/img: 0.0541 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 1.646 (epoch: 16, step: 0) // Avg time/img: 0.0262 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.62\u001b[0m %\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.891 (epoch: 17, step: 0) // Avg time/img: 0.0677 s\n",
            "loss: 1.754 (epoch: 17, step: 50) // Avg time/img: 0.0548 s\n",
            "loss: 1.766 (epoch: 17, step: 100) // Avg time/img: 0.0543 s\n",
            "loss: 1.764 (epoch: 17, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 1.763 (epoch: 17, step: 200) // Avg time/img: 0.0540 s\n",
            "loss: 1.763 (epoch: 17, step: 250) // Avg time/img: 0.0542 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 1.652 (epoch: 17, step: 0) // Avg time/img: 0.0200 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.41\u001b[0m %\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  5.000000000000001e-08\n",
            "loss: 1.572 (epoch: 18, step: 0) // Avg time/img: 0.0619 s\n",
            "loss: 1.746 (epoch: 18, step: 50) // Avg time/img: 0.0534 s\n",
            "loss: 1.755 (epoch: 18, step: 100) // Avg time/img: 0.0536 s\n",
            "loss: 1.753 (epoch: 18, step: 150) // Avg time/img: 0.0539 s\n",
            "loss: 1.756 (epoch: 18, step: 200) // Avg time/img: 0.0539 s\n",
            "loss: 1.762 (epoch: 18, step: 250) // Avg time/img: 0.0538 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 1.645 (epoch: 18, step: 0) // Avg time/img: 0.0224 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.39\u001b[0m %\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  5.000000000000001e-08\n",
            "loss: 1.701 (epoch: 19, step: 0) // Avg time/img: 0.0580 s\n",
            "loss: 1.774 (epoch: 19, step: 50) // Avg time/img: 0.0541 s\n",
            "loss: 1.776 (epoch: 19, step: 100) // Avg time/img: 0.0540 s\n",
            "loss: 1.772 (epoch: 19, step: 150) // Avg time/img: 0.0541 s\n",
            "loss: 1.773 (epoch: 19, step: 200) // Avg time/img: 0.0544 s\n",
            "loss: 1.767 (epoch: 19, step: 250) // Avg time/img: 0.0543 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 1.643 (epoch: 19, step: 0) // Avg time/img: 0.0238 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.58\u001b[0m %\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  5.000000000000001e-08\n",
            "loss: 1.711 (epoch: 20, step: 0) // Avg time/img: 0.0696 s\n",
            "loss: 1.781 (epoch: 20, step: 50) // Avg time/img: 0.0546 s\n",
            "loss: 1.765 (epoch: 20, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.762 (epoch: 20, step: 150) // Avg time/img: 0.0543 s\n",
            "loss: 1.765 (epoch: 20, step: 200) // Avg time/img: 0.0545 s\n",
            "loss: 1.766 (epoch: 20, step: 250) // Avg time/img: 0.0543 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 1.642 (epoch: 20, step: 0) // Avg time/img: 0.0221 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.55\u001b[0m %\n",
            "========== TRAINING FINISHED ===========\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\" --model=\"enet\" --num-epochs 20  --savedir \"enet_finetuned2\" --batch-size=10 --pretrained --decoder --loadWeights=\"enet.pth\"\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BiSeNet V2: Finetuning Void Classifier**"
      ],
      "metadata": {
        "id": "1fWS7NMMoRsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir ../cityscapes --model=\"bisenet\" --num-epochs 20   --savedir \"bisenet_finetuned\" --batch-size=16 --pretrained --decoder --loadWeights=\"bisenetv2.pth\"\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hsW0abqoc94",
        "outputId": "79f3d2c0-ee0c-4eaa-98d4-d76d45d73c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/train\n",
            "cur_epoch  not loaded\n",
            "best_score  not loaded\n",
            "state_dict  not loaded\n",
            "optimizer  not loaded\n",
            "scheduler  not loaded\n",
            "========== DECODER TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/train/main.py:625: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = load_my_state_dict(model.module, torch.load(weights_path))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 2.8149,  6.9850,  3.7890,  9.9428,  9.7702,  9.5111, 10.3114, 10.0265,\n",
            "         4.6323,  9.5608,  7.8698,  9.5169, 10.3737,  6.6616, 10.2605, 10.2879,\n",
            "        10.2898, 10.4054, 10.1381,  1.0000], device='cuda:0')\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/train/main.py:147: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n",
            "  self.loss = torch.nn.NLLLoss2d(weight)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 3.013 (epoch: 1, step: 0) // Avg time/img: 0.2496 s\n",
            "loss: 1.951 (epoch: 1, step: 50) // Avg time/img: 0.0566 s\n",
            "loss: 1.745 (epoch: 1, step: 100) // Avg time/img: 0.0551 s\n",
            "loss: 1.669 (epoch: 1, step: 150) // Avg time/img: 0.0546 s\n",
            "----- VALIDATING - EPOCH 1 -----\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/train/main.py:437: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
            "/content/train/main.py:438: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  targets = Variable(labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL loss: 1.403 (epoch: 1, step: 0) // Avg time/img: 0.0240 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m16.69\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.004774426908107499\n",
            "loss: 1.677 (epoch: 2, step: 0) // Avg time/img: 0.0584 s\n",
            "loss: 1.392 (epoch: 2, step: 50) // Avg time/img: 0.0551 s\n",
            "loss: 1.305 (epoch: 2, step: 100) // Avg time/img: 0.0554 s\n",
            "loss: 1.278 (epoch: 2, step: 150) // Avg time/img: 0.0556 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 1.262 (epoch: 2, step: 0) // Avg time/img: 0.0206 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.61\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.004547662880414811\n",
            "loss: 1.232 (epoch: 3, step: 0) // Avg time/img: 0.0598 s\n",
            "loss: 1.213 (epoch: 3, step: 50) // Avg time/img: 0.0559 s\n",
            "loss: 1.173 (epoch: 3, step: 100) // Avg time/img: 0.0560 s\n",
            "loss: 1.161 (epoch: 3, step: 150) // Avg time/img: 0.0559 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 1.35 (epoch: 3, step: 0) // Avg time/img: 0.0206 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.67\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0043196348615140955\n",
            "loss: 1.14 (epoch: 4, step: 0) // Avg time/img: 0.0602 s\n",
            "loss: 1.148 (epoch: 4, step: 50) // Avg time/img: 0.0545 s\n",
            "loss: 1.147 (epoch: 4, step: 100) // Avg time/img: 0.0544 s\n",
            "loss: 1.125 (epoch: 4, step: 150) // Avg time/img: 0.0543 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 1.073 (epoch: 4, step: 0) // Avg time/img: 0.0256 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m20.39\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.004090260730254292\n",
            "loss: 1.187 (epoch: 5, step: 0) // Avg time/img: 0.0608 s\n",
            "loss: 1.055 (epoch: 5, step: 50) // Avg time/img: 0.0534 s\n",
            "loss: 1.064 (epoch: 5, step: 100) // Avg time/img: 0.0536 s\n",
            "loss: 1.065 (epoch: 5, step: 150) // Avg time/img: 0.0536 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 1.049 (epoch: 5, step: 0) // Avg time/img: 0.0200 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.53\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0038594475336178524\n",
            "loss: 1.084 (epoch: 6, step: 0) // Avg time/img: 0.0567 s\n",
            "loss: 1.041 (epoch: 6, step: 50) // Avg time/img: 0.0537 s\n",
            "loss: 1.019 (epoch: 6, step: 100) // Avg time/img: 0.0543 s\n",
            "loss: 1.011 (epoch: 6, step: 150) // Avg time/img: 0.0548 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.9256 (epoch: 6, step: 0) // Avg time/img: 0.0266 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.08\u001b[0m %\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.0036270892346860996\n",
            "loss: 0.9443 (epoch: 7, step: 0) // Avg time/img: 0.0619 s\n",
            "loss: 0.9415 (epoch: 7, step: 50) // Avg time/img: 0.0554 s\n",
            "loss: 0.9691 (epoch: 7, step: 100) // Avg time/img: 0.0554 s\n",
            "loss: 0.9701 (epoch: 7, step: 150) // Avg time/img: 0.0556 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 1.046 (epoch: 7, step: 0) // Avg time/img: 0.0240 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.73\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0033930637962906254\n",
            "loss: 0.9125 (epoch: 8, step: 0) // Avg time/img: 0.0627 s\n",
            "loss: 0.948 (epoch: 8, step: 50) // Avg time/img: 0.0555 s\n",
            "loss: 0.949 (epoch: 8, step: 100) // Avg time/img: 0.0557 s\n",
            "loss: 0.9503 (epoch: 8, step: 150) // Avg time/img: 0.0557 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 1.055 (epoch: 8, step: 0) // Avg time/img: 0.0237 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.02\u001b[0m %\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.0031572293374467764\n",
            "loss: 0.9519 (epoch: 9, step: 0) // Avg time/img: 0.0616 s\n",
            "loss: 0.9408 (epoch: 9, step: 50) // Avg time/img: 0.0556 s\n",
            "loss: 0.9162 (epoch: 9, step: 100) // Avg time/img: 0.0559 s\n",
            "loss: 0.9129 (epoch: 9, step: 150) // Avg time/img: 0.0558 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 1.096 (epoch: 9, step: 0) // Avg time/img: 0.0271 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.25\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.0029194189645999013\n",
            "loss: 1.084 (epoch: 10, step: 0) // Avg time/img: 0.0598 s\n",
            "loss: 0.9091 (epoch: 10, step: 50) // Avg time/img: 0.0551 s\n",
            "loss: 0.8874 (epoch: 10, step: 100) // Avg time/img: 0.0556 s\n",
            "loss: 0.8813 (epoch: 10, step: 150) // Avg time/img: 0.0557 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.246 (epoch: 10, step: 0) // Avg time/img: 0.0233 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.20\u001b[0m %\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.002679433656340733\n",
            "loss: 1.136 (epoch: 11, step: 0) // Avg time/img: 0.0596 s\n",
            "loss: 0.8635 (epoch: 11, step: 50) // Avg time/img: 0.0557 s\n",
            "loss: 0.8708 (epoch: 11, step: 100) // Avg time/img: 0.0555 s\n",
            "loss: 0.8573 (epoch: 11, step: 150) // Avg time/img: 0.0556 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.8634 (epoch: 11, step: 0) // Avg time/img: 0.0281 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.98\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.002437032195894977\n",
            "loss: 0.7518 (epoch: 12, step: 0) // Avg time/img: 0.0589 s\n",
            "loss: 0.8298 (epoch: 12, step: 50) // Avg time/img: 0.0552 s\n",
            "loss: 0.8696 (epoch: 12, step: 100) // Avg time/img: 0.0554 s\n",
            "loss: 0.8611 (epoch: 12, step: 150) // Avg time/img: 0.0555 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.8107 (epoch: 12, step: 0) // Avg time/img: 0.0248 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.87\u001b[0m %\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0021919164527704348\n",
            "loss: 1.018 (epoch: 13, step: 0) // Avg time/img: 0.0631 s\n",
            "loss: 0.8101 (epoch: 13, step: 50) // Avg time/img: 0.0557 s\n",
            "loss: 0.8153 (epoch: 13, step: 100) // Avg time/img: 0.0557 s\n",
            "loss: 0.8179 (epoch: 13, step: 150) // Avg time/img: 0.0557 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.8563 (epoch: 13, step: 0) // Avg time/img: 0.0311 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.97\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0019437089939938173\n",
            "loss: 0.9816 (epoch: 14, step: 0) // Avg time/img: 0.0601 s\n",
            "loss: 0.8127 (epoch: 14, step: 50) // Avg time/img: 0.0556 s\n",
            "loss: 0.7899 (epoch: 14, step: 100) // Avg time/img: 0.0560 s\n",
            "loss: 0.7859 (epoch: 14, step: 150) // Avg time/img: 0.0559 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.821 (epoch: 14, step: 0) // Avg time/img: 0.0235 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.88\u001b[0m %\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0016919173095082494\n",
            "loss: 0.7713 (epoch: 15, step: 0) // Avg time/img: 0.0593 s\n",
            "loss: 0.7876 (epoch: 15, step: 50) // Avg time/img: 0.0552 s\n",
            "loss: 0.7995 (epoch: 15, step: 100) // Avg time/img: 0.0555 s\n",
            "loss: 0.8047 (epoch: 15, step: 150) // Avg time/img: 0.0555 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.8568 (epoch: 15, step: 0) // Avg time/img: 0.0249 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.86\u001b[0m %\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0014358729437462937\n",
            "loss: 0.8611 (epoch: 16, step: 0) // Avg time/img: 0.0617 s\n",
            "loss: 0.7856 (epoch: 16, step: 50) // Avg time/img: 0.0558 s\n",
            "loss: 0.7688 (epoch: 16, step: 100) // Avg time/img: 0.0559 s\n",
            "loss: 0.7721 (epoch: 16, step: 150) // Avg time/img: 0.0558 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.6379 (epoch: 16, step: 0) // Avg time/img: 0.0322 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.00\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.0011746189430880188\n",
            "loss: 0.6868 (epoch: 17, step: 0) // Avg time/img: 0.0598 s\n",
            "loss: 0.7694 (epoch: 17, step: 50) // Avg time/img: 0.0559 s\n",
            "loss: 0.7773 (epoch: 17, step: 100) // Avg time/img: 0.0559 s\n",
            "loss: 0.7747 (epoch: 17, step: 150) // Avg time/img: 0.0557 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.9103 (epoch: 17, step: 0) // Avg time/img: 0.0261 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.66\u001b[0m %\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0009066760365683728\n",
            "loss: 0.6558 (epoch: 18, step: 0) // Avg time/img: 0.0604 s\n",
            "loss: 0.7792 (epoch: 18, step: 50) // Avg time/img: 0.0551 s\n",
            "loss: 0.7713 (epoch: 18, step: 100) // Avg time/img: 0.0554 s\n",
            "loss: 0.7705 (epoch: 18, step: 150) // Avg time/img: 0.0555 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.8147 (epoch: 18, step: 0) // Avg time/img: 0.0234 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.24\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.0006294627058970835\n",
            "loss: 0.7291 (epoch: 19, step: 0) // Avg time/img: 0.0565 s\n",
            "loss: 0.7665 (epoch: 19, step: 50) // Avg time/img: 0.0560 s\n",
            "loss: 0.7373 (epoch: 19, step: 100) // Avg time/img: 0.0560 s\n",
            "loss: 0.7432 (epoch: 19, step: 150) // Avg time/img: 0.0559 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.8783 (epoch: 19, step: 0) // Avg time/img: 0.0206 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_finetuned/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.0003373207119183911\n",
            "loss: 0.6058 (epoch: 20, step: 0) // Avg time/img: 0.0650 s\n",
            "loss: 0.7043 (epoch: 20, step: 50) // Avg time/img: 0.0558 s\n",
            "loss: 0.7381 (epoch: 20, step: 100) // Avg time/img: 0.0561 s\n",
            "loss: 0.7357 (epoch: 20, step: 150) // Avg time/img: 0.0560 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.7038 (epoch: 20, step: 0) // Avg time/img: 0.0261 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.21\u001b[0m %\n",
            "========== TRAINING FINISHED ===========\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: CE + LN**"
      ],
      "metadata": {
        "id": "9wu8HXXRtTD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\" --num-epochs 30 --savedir \"erfnet_finetuned_CE_logit_norm\" --batch-size=12 --pretrained --decoder --logit-norm\n",
        "%cd .."
      ],
      "metadata": {
        "id": "HVT97Jt0tmmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: Focal Loss**"
      ],
      "metadata": {
        "id": "-XAOfwhWaqGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\"  --num-epochs 30 --savedir \"erfnet_focal_loss\" --batch-size=12 --pretrained --decoder --loss=\"focal_loss\"\n",
        "%cd .."
      ],
      "metadata": {
        "id": "AmCS_rcsalwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: Focal Loss + Logit Norm**"
      ],
      "metadata": {
        "id": "1875VK4vazjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\"  --num-epochs 30 --savedir \"erfnet_focal_loss_logit_norm\" --batch-size=12 --pretrained --decoder --loss=\"focal_loss\" --logit-norm\n",
        "%cd .."
      ],
      "metadata": {
        "id": "iunWlWVaawae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: IsoMax Plus Loss + CE**"
      ],
      "metadata": {
        "id": "3A3x0LQf7TvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\"  --num-epochs 30 --savedir \"erfnet_isomax_ce\" --model=\"erfnet_isomax\" --batch-size=12 --pretrained --decoder --loss=\"cross_entropy\"\n",
        "%cd .."
      ],
      "metadata": {
        "id": "ADgtq2tn7OEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: IsoMax Plus Loss + FL**"
      ],
      "metadata": {
        "id": "HlYsrnVGgs0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\"  --num-epochs 30 --savedir \"erfnet_isomax_focal_loss\" --model=\"erfnet_isomax\" --batch-size=12 --pretrained --decoder --loss=\"focal_loss\"\n",
        "%cd .."
      ],
      "metadata": {
        "id": "6XA-b2jhgzQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: Dice Loss**"
      ],
      "metadata": {
        "id": "RHf-_BwXhDBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\"  --num-epochs 30 --savedir \"erfnet_dice_loss\" --model=\"erfnet\" --batch-size=12 --pretrained --decoder --loss=\"dice_loss\"\n",
        "%cd .."
      ],
      "metadata": {
        "id": "zxzKuVqChLOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: Dice Loss + CE**"
      ],
      "metadata": {
        "id": "ISbzw5QjhRSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\"  --num-epochs 30 --savedir \"erfnet_ce_dice_loss\" --model=\"erfnet\" --batch-size=12 --pretrained --decoder --loss=\"CE_dice_loss\"\n",
        "%cd .."
      ],
      "metadata": {
        "id": "Siykt_9PhXS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ERFNET: Dice Loss + FL**"
      ],
      "metadata": {
        "id": "C5jEA6SehSOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train\n",
        "%run main.py --datadir \"../cityscapes/\"  --num-epochs 30 --savedir \"erfnet_focal_loss_dice_loss\" --model=\"erfnet\" --batch-size=12 --pretrained --decoder --loss=\"focal_loss_dice_loss\"\n",
        "%cd .."
      ],
      "metadata": {
        "id": "m-RR85Oahmgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}